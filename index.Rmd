---
title: "My Portfolio"
author: "Anne Leendertse"
date: "Spring 2023"
output:
  flexdashboard::flex_dashboard:
    storyboard: true
---

```{r}
library(tidyverse)
library(spotifyr)
library(plotly)
library(compmus)
library(tidyr)
library(reshape2)
library(shiny)
```

```{r}
andy_shauf <- get_track_audio_features("15WJWyrI3c6aRuvbYgMcKv")
my_portfolio <- get_playlist_audio_features("", "3MxAsxThsrUCjU4QAplkg0")
my_portfolio2 <- get_playlist_audio_features("", "0nYReIA8L6ep3yZp00TLwf")
Home <- get_track_audio_features("2YonDH4haZzgbo3NjpIalE")

Home_num <- Home %>% select_if(is.numeric) %>% select(-time_signature, -key, -tempo, -duration_ms, -loudness)
andy_shauf_num <- andy_shauf %>% select_if(is.numeric)

my_portfolio2_num <- my_portfolio2 %>% select_if(is.numeric)
my_portfolio2_clean <- my_portfolio2_num %>% select(-track.disc_number, -track.disc_number, -track.popularity, -track.track_number, -track.album.total_tracks, -time_signature, -key, -tempo, -track.duration_ms, -loudness)

my_portfolio_clean_mean <- apply(my_portfolio2_clean, 2, mean) 

compare_influence_writing <- rbind(my_portfolio_clean_mean, Home_num)

compare_influence_writing <- t(compare_influence_writing )


colnames(compare_influence_writing)[2]  <- "My song" 
colnames(compare_influence_writing)[1]  <- "Mean of influence
songs" 

jahe <- data.frame(compare_influence_writing)




jahe.long <- melt(jahe)

namen <- rownames(jahe)
variables <- append(namen, rownames(jahe))


```
New tab (1)
=====================================


```{r}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )


Home_keygram <-
  get_tidy_audio_analysis("2YonDH4haZzgbo3NjpIalE") |>
  compmus_align(beats, segments) |>
  select(beats) |>
  unnest(beats) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

Home_keygram2 <-
  get_tidy_audio_analysis("2YonDH4haZzgbo3NjpIalE") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )


```


Overview {data-width=500}
--------------------------------------------------

### Overview

A chordogram is a visual representation within music analysis. It tries to capture the harmonies within a piece and sets it out in time. 

On the right side of this tab, you can see a chordogram and a keygram, both of my piece. The first one is divided into beats, the second divided into sections. 

For me, it looks like there's a bit of a struggle making these chordograms for this particular song. The song verse of the song makes use of the following chords, switching every two beats:

**Gmaj** - **Cmin** - **Abmaj** - **C#min** - **Cmin** - **Abmaj** - **Cmin** - **Cmin**

This pattern happens twice in a verse. When looking at the chordogram, it doesn't fully seem to pick up on these chords. However, it gets really close and sometimes it indeed get's it right. **C#min** for example, get's recogised every single time it occurs in the song. This is really interesting to me. 







column2 {data-width=500}
--------------------------------------------------

### State of Unrest - Self-similarity Matrix (Chroma)

```{r}
Home_keygram |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```



### State of Unrest - Self-similarity Matrix (Timbre)

```{r}
Home_keygram2 |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```





New tab (2)
=====================================


column_test {data-width=750}
--------------------------------------------------

### Nice Plot
```{r}
MeestBeluisterd2022 <-
  get_playlist_audio_features(
    "thesoundsofspotify",
    "0nYReIA8L6ep3yZp00TLwf"
  ) |>
  slice(1:30) |>
  add_audio_analysis()

Home <- 
  get_playlist_audio_features(
    "thesoundsofspotify",
    "2OKDjAF0WFX2OntThq1N6f"
  ) |>
  slice(1:30) |>
  add_audio_analysis()
  

NummerEnBeluisterd <-
  MeestBeluisterd2022 |>
  mutate(genre = "MeestBeluisterd2022") |>
  bind_rows(Home |> mutate(genre = "Home"))


NummerEnBeluisterd |>
  mutate(
    timbre =
      map(
        segments,
        compmus_summarise,
        timbre,
        method = "mean"
      )
  ) |>
  select(genre, timbre) |>
  compmus_gather_timbre() |>
  filter(basis %in% c("c01", "c02", "c03", "c04", "c05", "c06")) |> 
  ggplot(aes(x = basis, y = value, fill = genre)) +
  geom_violin() +
  scale_fill_viridis_d() +
  labs(x = "Spotify Timbre Coefficients", y = "", fill = "Genre")

```



column_test2 {data-width=250}
--------------------------------------------------


### Tekst
Hier komt dan de tekst pikkie





Old tabs {.storyboard}
=====================================

### Chromatogram

#### The Chromagram and cepstogram of my piece:

```{r}
Home_tidy <-
  get_tidy_audio_analysis("2YonDH4haZzgbo3NjpIalE") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

Home_tidy |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()

Home_Cepstogram <-
  get_tidy_audio_analysis("2YonDH4haZzgbo3NjpIalE") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

Home_Cepstogram |>
  compmus_gather_timbre() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +                              
  theme_classic()
  

```

------------------------------------------------------------------------

On the **left side** you can see the chromagram of my song "Home". Overall it looks pretty clear to distinguish the different pitch classes that are present in the piece. Although I would say the piece is not in one particular key, the verses are in Cminor. The first chorus starts around 50 seconds into the song, and there you can see the C is no longer prevalent in the song. In the outro of the song, starting around 120 seconds in, the song rests on the Cminor chord for a long time and that can be clearly extracted from this chromatogram. 

Then on the **right side** a cepstogram of my song can be seen. I found that by using a "euclidean" normalisation  and the "root mean square" as a summary statistic, I got the clearest representation. There are three things that to me are interesting to see in this cepstogram.

- In the first layer c01, which is basically the loudness of the piece at a given time, you can see that on the far left- and right side, the song is very quiet. You can recognise this by the darkness of the blocks. 
- In the second layer c02, which shows the presence of lower frequencies (bass), the sections between 0 and 25 seconds and between 60 and 80 seconds look brighter. I can't really explain it that well, other than that it could be assigned to the fact that the loudness of the bass is there the highest, relative to the rest of the piece.
- In the third layer c03, which expresses the presence of the mids, you can see that between 25 and 50 seconds in this layer gets brighter. I think this is because that's where the vocals come in. 


### Self-similarity matrice (New tab)

#### The Self-similarity matrice of my song:

```{r}
Home_self_sim <-
  get_tidy_audio_analysis("2YonDH4haZzgbo3NjpIalE") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )


bind_rows(
  Home_self_sim |>
    compmus_self_similarity(pitches, "euclidean") |>
    mutate(d = d / max(d), type = "Chroma"),
  Home_self_sim |>
    compmus_self_similarity(timbre, "euclidean") |>
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |>
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "")

```

***
On this tab, you can see both the chroma-based and timbre-based self-similarity matrices of my song. Again, for both I liked using the "euclidean" and the "root mean square" the most. Now I will describe what I see in both matrices.

##### **Chroma-based**
So based on the looking at the left matrix, you can generally distinguish three different kind of sections. 
- Verses: Between both 0 and 45 seconds and 65 and 110 seconds you can se repeating checkerboard like patterns. These are the verses. They make use of the same chord progressions. 
- Choruses: Between 45 and 65 and between 110 and 130 you can see the choruses. Although they don't really show diagonals clearly, the same pattern can be seen in both sections.
- Outro: From around 130 seconds in, the outro starts. This is represented in the top right of the matrix and a clear black checkerboard box can be seen. 




### Introduction

(I just changed my topic 2 days ago, so let me know what you think.)

Just three days ago, the debut single, "Home", of my band was released on Spotify, marking a significant moment in my journey as a songwriter. It's the first time that the public can openly listen to my music, and I'm feeling a mix of excitement and nerves as I await the response from listeners.

But the release of my song on Spotify also offers new possibilities beyond just reaching a wider audience. With the help of the Spotify API, I can analyze my song and gain insights into its musical characteristics.

As a songwriter, I'm always fascinated by the creative process and how other music shapes the songs we write. For this project, I'm curious to explore the ways in which different artists and genres have influenced my own music.

To do this, I plan to compare my debut song to my top songs of 2022 and identify any correlations or similarities. Which artists or songs have had the greatest impact on my writing, and have I unintentionally incorporated elements of other music into my work. In other words: have I maybe accidentally ripped someone off?

As a songwriter, I believe that understanding the creative influences behind our music can enhance our artistry and lead to new discoveries. This project offers a unique opportunity to delve deeper into the musical elements that shape my writing and explore the ways in which other artists and genres have impacted my work. I'm excited to embark on this journey of self-discovery and to share my findings with fellow music enthusiasts.

If you stumble upon this dashboard and you would like to see what the song, "Home", sounds like, here is the Spotify link: <https://open.spotify.com/track/2YonDH4haZzgbo3NjpIalE?si=324e6a2d4e424358>

### Comparing numerical variables

```{r}
ding <- ggplot(jahe.long,aes(x=variables,value,fill=variable))+
     geom_bar(stat="identity",position="dodge") + theme(axis.text.x = element_text(angle = 90))

ggplotly(ding)
```

------------------------------------------------------------------------

In the bar graph in red you can see the mean for all the variables of my 101 top songs of 2022. These variables are the ones that are ranked between 0 and 1 by Spotify. In blue you see these numbers for my song. They differ the most for accounstichness.



