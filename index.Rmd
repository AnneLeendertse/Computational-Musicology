---
title: "Inspiration on songwriting"
author: "Anne Leendertse"
date: "Spring 2023"
output:
  flexdashboard::flex_dashboard:
    storyboard: true
    self_contained: false
    theme: 
      version: 4
      bootswatch: minty
---

```{r}
library(tidyverse)
library(spotifyr)
library(plotly)
library(compmus)
library(tidyr)
library(reshape2)
library(shiny)
library(GGally)
library(patchwork)

```

```{r}
andy_shauf <- get_track_audio_features("15WJWyrI3c6aRuvbYgMcKv")
my_portfolio <- get_playlist_audio_features("", "3MxAsxThsrUCjU4QAplkg0")
my_portfolio2 <- get_playlist_audio_features("", "0nYReIA8L6ep3yZp00TLwf")
my_portfolio2$mode <- factor(my_portfolio2$mode)
levels(my_portfolio2$mode) <- c("minor", "major")
Home <- get_track_audio_features("2YonDH4haZzgbo3NjpIalE")

Home_num <- Home %>% select_if(is.numeric) %>% select(-time_signature, -key, -tempo, -duration_ms, -loudness)
andy_shauf_num <- andy_shauf %>% select_if(is.numeric)

my_portfolio2_num <- my_portfolio2 %>% select_if(is.numeric)
my_portfolio2_clean <- my_portfolio2_num %>% select(-track.disc_number, -track.disc_number, -track.popularity, -track.track_number, -track.album.total_tracks, -time_signature, -key, -tempo, -track.duration_ms, -loudness)

my_portfolio_key <- my_portfolio2 %>% select(key, mode)

my_portfolio_tempo <- my_portfolio2 %>% select(tempo, track.name)



my_portfolio_clean_mean <- apply(my_portfolio2_clean, 2, mean) 
compare_influence_writing <- rbind(my_portfolio_clean_mean, Home_num)
compare_influence_writing <- t(compare_influence_writing )




colnames(compare_influence_writing)[2]  <- "Home" 
colnames(compare_influence_writing)[1]  <- "Influence" 

jahe <- data.frame(compare_influence_writing)




jahe.long <- melt(jahe)

namen <- rownames(jahe)
variables <- append(namen, rownames(jahe))


```

Introduction
=====================================

Introduction {data-width=500}
--------------------------------------------------

### Introduction

Just last month, the debut single, "Home", of my band was released on Spotify, marking a significant moment in my journey as a songwriter. It's the first time that the public can openly listen to my music, and I'm feeling a mix of excitement and nerves as I await the response from listeners.

But the release of my song on Spotify also offers new possibilities beyond just reaching a wider audience. With the help of the Spotify API, I can analyze my song and gain insights into its musical characteristics.

As a songwriter, I'm always fascinated by the creative process and how other music shapes the songs we write. For this project, I'm curious to explore the ways in which different artists and genres have influenced my own music.

To do this, I plan to compare my debut song to my top songs of 2022 and identify any correlations or similarities. Which artists or songs have had the greatest impact on my writing, and have I unintentionally incorporated elements of other music into my work. In other words: have I maybe accidentally ripped someone off?

As a songwriter, I believe that understanding the creative influences behind our music can enhance our artistry and lead to new discoveries. This project offers a unique opportunity to delve deeper into the musical elements that shape my writing and explore the ways in which other artists and genres have impacted my work. 

Before making a comparison, I want to analyze the songs that I listened to the most in 2022 with the aim of identifying any interesting trends or correlations. This section can be accessed by clicking the **"Inspiration feature analysis"** button at the top of the page. Next, I will turn my attention to my own song, which can be found by clicking on the **"Audio analysis of Home"** button. Finally, in the last section, I will seek to make comparisons and identify the song that Spotify suggests is most similar to my own. You can find this by clicking on **"Comparing inspiration and Home"**

If you want to listen to what my song "Home" and the songs that could have influenced me sound like, look on the right side of this tab.


embededspotify {data-width=500}
--------------------------------------------------

### Home {.myclass style="height:100px;"}

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/2YonDH4haZzgbo3NjpIalE?utm_source=generator" width="100%" height="352" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>


### My top songs of 2022


<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/0nYReIA8L6ep3yZp00TLwf?utm_source=generator" width="100%" height="352" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>



Inspiration feature analysis {.storyboard}
=====================================

### Introduction

In this section, I will be doing a feature analysis on the songs I listened to the most in 2022. Analyzing your most listened-to songs can provide valuable insights into your music preferences and habits, potentially revealing interesting trends and correlations. For example, you may notice that you tend to listen to songs with a high bpm, or songs that Spotify categorizes as "sad". Overall, analyzing your most listened-to songs in 2022 can be very interesting. Looking into the songs that I those might give me some insight into why my music sounds the way that it does.


### Acousticness, Energy, Loudness and Mode

```{r Acousticness, Energy, Loudness and Mode}
# Create a scatterplot
first_scatter <- ggplot(my_portfolio2, aes(x=energy, y=acousticness, size=loudness, text=track.name, color = mode)) +
  geom_point(alpha=0.7) +

  # Add titles and labels to axes
  ggtitle("Playlist Analysis") +
  xlab("Energy") +
  ylab("Acousticness") +
  
  # Customize colors and theme
  scale_color_brewer(palette = "Set1") +
  theme_minimal() + guides(loudness=FALSE)

lm <- lm(acousticness ~ energy, data = my_portfolio2)
first_scatter <- first_scatter + geom_abline(intercept = lm$coefficients[1], slope = lm$coefficients[2]) + theme_bw() + guides(loudness=FALSE)


# Convert the scatterplot to a plotly object and display
ggplotly(first_scatter, tooltip = c("text"), hoverinfo = "text") 

```

***

This plot displays a scatter plot depicting the relationship between Energy and Acousticness of all songs in my corpus. A downwards trend is visible between the two variables. To show this trend more clearly, I plotted a line following the data. Songs in my corpus that have higher levels of acousticness tend to have lower levels of energy, and vice versa. This seems logical since acoustic songs generally have less energy than non-acoustic songs.

The size of the dots shows the loudness of the song. The louder the song, the bigger the dot is shown in the plot. You can see that acoustic songs also seem to be a bit less loud. I think there are also some interesting cases when it comes to loudness. First of all, in the top left you see a very small dot. This is the song "Quarters" by American indie/folk band "Wilco". This song seems to be the quietest song by far; a real outlier. Secondly, there is a song called "Reelin' in the years" by the band "Steely Dan" (The song sits around 0.55 Energy and 0.20 Acousticness). It seems to be a lot less loud than all of the songs around it. I think this might be due to the fact that this song, unlike most of the songs in this corpus, is produced in the early 70s. Songs that are mastered a long time ago sometimes are a bit less loud. 

The dots in the plot are color-coded to represent the mode of the song, which indicates whether it is in a major or minor key. While there does not appear to be a correlation between mode and the other variables, the plot does reveal that there are more songs in major key than in minor key in my corpus.


### Distribution of keys
```{r}

note_labels <- c("C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B")

histogram1 <- ggplot(data = my_portfolio_key, aes(x = key, text = mode)) + geom_histogram(binwidth = 1, fill = "indianred1", color = "indianred4", alpha = 1, ) + scale_x_continuous(breaks = 0:11, labels = note_labels) +
  scale_fill_manual(values = c("green3", "indianred1"), 
                    name = "Mode", 
                    labels = c("Major", "Minor")) + labs(title = "Histogram Key", x = "Key", y = "Frequency") + theme_bw()


ggplotly(histogram1,  tooltip = c("key","text", "count"))
```


***

This histogram displays the distribution of the keys among the songs that I listened to the most in 2022. As expected, there is not really a certain key that I gravitate to the most. I don't have perfect pitch, so I wouldn't be able to know by just listening to a song what key it is in. What you can conclude, is that I lean more towards songs in major. 

However, some people state that the key of a song does in fact effect the way it feels. Take a look at this for example: https://takelessons.com/blog/how-music-affects-emotions#.

Another thing that could effect this distribution, is what key musicians tend to gravitate towards. When using a normal tuning and standard chords on a guitar for example, maybe they write songs more frequently in Cmaj. 




### Distribution of tempo

```{r}
index_max_tempo <- which.max(my_portfolio_tempo$tempo)
track_name_max_tempo <- my_portfolio_tempo$track.name[index_max_tempo]

index_min_tempo <- which.min(my_portfolio_tempo$tempo)
track_name_min_tempo <- my_portfolio_tempo$track.name[index_min_tempo]



histogram1 <- ggplot(data = my_portfolio_tempo, aes(x = tempo)) + geom_histogram(binwidth = 2, fill = "steelblue1", color = "steelblue4", alpha = 1) + labs(title = "Histogram Tempo", x = "Tempo", y = "Frequency") + theme_bw() + annotate("text", x = 124, y = 8, label = "Most frequent tempo", color = "steelblue4") + annotate("text", x = 124, y = 7.5, label = "124bpm", color = "steelblue4") + annotate("text", x = 189, y = 2, label = track_name_max_tempo, size = 2 , color = "steelblue1") + annotate("text", x = 189, y = 2.2, label = "192bpm", size = 2 , color = "steelblue1") + annotate("text", x = 65, y = 3, label = track_name_min_tempo, size = 2 , color = "steelblue1") + annotate("text", x = 64, y = 3.2, label = "64bpm", size = 2 , color = "steelblue1")

ggplotly(histogram1)

```

***

In this image, you can see a histogram that displays the different tempo's of the songs that I have listened to the most. This graph makes clear that I listen to music with a high variety of tempo. The bpm varies between 64 and 192. 

 What's interesting is that 124 bpm seems to be my preferred tempo by a significant margin, which makes sense given that it falls within the most popular bpm range of 120 to 130 bpm according to research (http://hdl.handle.net/1854/LU-159578). However, there doesn't seem to be a clear preference for other tempos within this range.
 
At first, I thought that there was probably no way that the two songs at both end of the range of tempos really were 64 and 192 bpm. This actually seemed to be the case. **"Even I can see"** is a very slow and beautiful song by **"Jeff Tweedy"**, front man of the band **"Wilco"**. And the fastest song, **"Mysterious river snake"** actually is very fast. It's also not in a standard meter, 3/4 or 6/8, depending how you listen to it. You can listen to the songs here:

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/2SRpWh6gc8stfB3hKuhyi6?utm_source=generator" width="50%" height="152" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/7m38lSSQsmMDcAhRPKbU2G?utm_source=generator" width="50%" height="152" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>

### Loudness and tempo, using Kmeans

```{r}
# Assuming your data is stored in a data frame called "music_data"
set.seed(123)  # For reproducibility

my_portfolio2_kmeans <- my_portfolio2 %>% select(tempo, loudness)

my_portfolio2_kmeans <- my_portfolio2_kmeans %>%
  mutate(tempo_rescaled = scale(tempo, center = min(tempo), scale = diff(range(tempo))))

my_portfolio2_kmeans <- my_portfolio2_kmeans %>%
  mutate(loudness_rescaled = scale(loudness, center = min(loudness), scale = diff(range(loudness))))

my_portfolio2_kmeans <- my_portfolio2_kmeans %>% select(tempo_rescaled, loudness_rescaled)


# making sure the names of the artists appear
flippedup <- my_portfolio2 %>% select(track.artists)
names_list <- map(flippedup$track.artists, ~ .x$name)

# making sure the names of the songs appear
dippedup <- my_portfolio2 %>% select(track.name, tempo, loudness)

# Perform k-means clustering with k = 3 clusters
kmeans_result <- kmeans(my_portfolio2_kmeans, centers = 3)
my_portfolio2_clusters <- data.frame(my_portfolio2_kmeans, cluster = kmeans_result$cluster)

my_portfolio2_clusters$names <- names_list

my_portfolio2_clusters <- cbind(my_portfolio2_clusters, dippedup)

# Print the cluster assignments
testjebro <- ggplot(my_portfolio2_clusters, aes(x = tempo_rescaled, y = loudness_rescaled, color = factor(cluster), text=names, text2=track.name, text3 = tempo, text4 = loudness)) +
  geom_point(aes(shape = ifelse(names == "Loving", "triangle", "circle"))) +
  labs(x = "tempo", y = "Loudness", color = "Cluster") + theme_bw() + guides(shape=FALSE)


ggplotly(testjebro, tooltip = c("text", "text2", "text3", "text4"))

```

***

In this scatter plot, I wanted to see if there was a relation between tempo and loudness in my most listened songs. I ended up using k-means to create three clusters different clusters based on these parameters. To do this, I scaled both parameters to between 0 and 1. When you hover over the dots, you can see the artist and track names. 

The band that dominated my listening in 2022 is **"Loving"**, whose name surprisingly shares a similarity with my own band's name (although it was not intentional). Through this plot, I aimed to explore if their tracks were relatively consistent in tempo and loudness, or if they exhibited notable variability. Their tracks are distinguishable by the distinctive triangle shapes. Their songs actually seem to be quit varied, although there's still some ifs and buts. When you look at the song **"If I'm only my thoughts"** you can see that Spotify recognised their song as 189 bpm, but this is incorrect. I think it's actually half of it and that Spotify accidently marked the tempo octave as it's tempo. If you want to check the song out for yourself, this is it:

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/6N7nnLe36PK6vhvuOrk6JD?utm_source=generator" width="50%" height="152" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>

Audio analysis of "Home" {.storyboard}
=====================================

### Introduction

In this section, I will conduct an audio analysis of my original song, **"Home"**. The analysis will include a range of visualizations, including a chromagram, cepstogram, self-similarity matrices, chordogram, keygram, and tempogram. As the song's creator and producer, I have listened to it countless times throughout the recording and mixing process. This familiarity will allow me to interpret the audio analysis provided by Spotify with greater ease and potentially identify any errors or discrepancies. By examining how Spotify analyzes my track, I hope to gain a deeper understanding of its unique audio characteristics and identify any areas for improvement.


### Chromagram of "Home"

```{r}
Home_tidy <-
  get_tidy_audio_analysis("2YonDH4haZzgbo3NjpIalE") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

Home_tidy |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()
```

***

This is an image of the chromagram for my song "Home". A chromagram displays which pitch classes are present in the song at any given moment. The brightest areas represent the pitch class that is most prominent. For the norm for the chroma vectors, which represent the distribution of musical pitch classes in a song, I've chosen "euclidean". This because it seemed to generate the clearest result. 

Overall it looks pretty clear to distinguish the different pitch classes that are present in the piece. Although I would say the entire piece is not in only one key, the **verses** are in Cminor. Hence the fact that the C pitch class is so clearly represented in this image. The first **chorus** starts around 50 seconds into the song, and there you can see the C is no longer prevalent in the song. In the outro of the song, starting around 120 seconds in, the song rests on the Cminor chord for a long time and that can be clearly extracted from this chromagram. 


### Cepstogram of "Home"

```{r}
Home_Cepstogram <-
  get_tidy_audio_analysis("2YonDH4haZzgbo3NjpIalE") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

Home_Cepstogram |>
  compmus_gather_timbre() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +                              
  theme_classic()
```

***

Here a cepstogram of my song can be seen. A cepstogram gives information about the timbre features (c01-c12) of a song. I found that by using a "euclidean" normalisation  and the "root mean square" as a summary statistic, I got the clearest representation. There are three things that to me are interesting to see in this cepstogram:

- In the first layer c01, which is basically the loudness of the piece at a given time, you can see that on the far left- and right side, the song is very quiet. You can recognise this by the darkness of the blocks.  
- In the second layer c02, which shows the presence of lower frequencies (bass), the sections between 0 and 25 seconds and between 60 and 80 seconds (verses) look brighter. I think that this might be the case, because the loudness of the bass is there the highest, relative to the rest of the piece. Also, the bass plays some higher notes during the chorusses of the song (first one starts around 50 seconds).
- In the third layer c03, which expresses the presence of the mids, you can see that between 25 and 50 seconds in this layer gets brighter. I think this is because that's where the vocals come in. 



### Self-similarity matrices of "Home"

```{r}
Home_self_sim <-
  get_tidy_audio_analysis("2YonDH4haZzgbo3NjpIalE") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )


bind_rows(
  Home_self_sim |>
    compmus_self_similarity(pitches, "euclidean") |>
    mutate(d = d / max(d), type = "Chroma"),
  Home_self_sim |>
    compmus_self_similarity(timbre, "euclidean") |>
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |>
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "")

```

***

On this tab, you can see both the chroma-based and timbre-based self-similarity matrices of my song. Self-similarity matrices can show you the structure of a song. Again, for both I found that using the "euclidean", worked best, and the "root mean square" the most. Now I will describe what I see in both matrices.

##### **Chroma-based**
So based on the looking at the left matrix, you can sort of distinguish three different kind of sections. 

- **Verses**: Between both 0 and 45 seconds and 65 and 110 seconds you can se repeating checkerboard like patterns. These are the verses. They make use of the same chord progressions.

- **Choruses**: Between 45 and 65 and between 110 and 130 you can see the choruses. Although they don't really show diagonals clearly, the same pattern can be seen in both sections.

- **Outro**: From around 130 seconds in, the outro starts. This is represented in the top right of the matrix and a clear black checkerboard box can be seen. 

You also can see that the piece repeats itself after the first chorus, because new parallel diagonal lines start from around 65 seconds.  

##### **Timbre-based**
So based on the looking at the right matrix, I think you can distinguish more than three sections. 

I believe that the reason for the difference between this matrix and the chroma-based one lies in the fact that the verses consist of two distinct parts. The verse is the same chord progression repeated 2 times, but the first in the first loop, there are no vocals, but rather a high pitched melody played on a synthesizer. The timbre is therefore very different between the two repetitions of the verse. 


### Chordo- and keygram of "Home"
```{r}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )


Home_keygram <-
  get_tidy_audio_analysis("2YonDH4haZzgbo3NjpIalE") |>
  compmus_align(beats, segments) |>
  select(beats) |>
  unnest(beats) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

Home_keygram2 <-
  get_tidy_audio_analysis("2YonDH4haZzgbo3NjpIalE") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
```


```{r}
# Home_keygram |>
#   compmus_match_pitch_template(
#     chord_templates,         # Change to chord_templates if descired
#     method = "euclidean",  # Try different distance metrics
#     norm = "manhattan"     # Try different norms
#   ) |>
#   ggplot(
#     aes(x = start + duration / 2, width = duration, y = name, fill = d)
#   ) +
#   geom_tile() +
#   scale_fill_viridis_c(guide = "none") +
#   theme_minimal() +
#   labs(x = "Time (s)", y = "")
# 
# Home_keygram2 |>
#   compmus_match_pitch_template(
#     chord_templates,         # Change to chord_templates if descired
#     method = "euclidean",  # Try different distance metrics
#     norm = "manhattan"     # Try different norms
#   ) |>
#   ggplot(
#     aes(x = start + duration / 2, width = duration, y = name, fill = d)
#   ) +
#   geom_tile() +
#   scale_fill_viridis_c(guide = "none") +
#   theme_minimal() +
#   labs(x = "Time (s)", y = "")


combined_plots <- Home_keygram |>
  compmus_match_pitch_template(
    chord_templates,
    method = "euclidean",
    norm = "manhattan"
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "") +
  plot_layout(ncol = 2, widths = c(5,5)) +
  Home_keygram2 |>
  compmus_match_pitch_template(
    key_templates,
    method = "euclidean",
    norm = "manhattan"
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")

combined_plots <- combined_plots + plot_layout(ncol = 2, widths = c(50, 50))

combined_plots
```

***

In this tab, you can see a chordogram (left) and a keygram (right) of my piece. The chordogram can show you what chord is being played at a given time and the keygram expresses per section what key it's in. 

##### **Chordogram**

I tried dividing the chordogram in beats, because the chords in my song change every two beats. To me, it looks like there's a bit of a struggle making these chordograms for this particular song. The song verse of the song makes use of the following chords: -->

**Gmaj** - **Cmin** - **Abmaj** - **C#min** - **Cmin** - **Abmaj** - **Cmin** - **Cmin**

This pattern happens twice in a verse. When looking at the chordogram, it doesn't fully seem to pick up on these chords. However, it gets really close and sometimes it indeed get's it right. **C#min** for example, get's recogised every single time it occurs in the song. This is really interesting to me.

##### **Keygram**

The keygram appears to outperform the chordogram, accurately identifying that the bulk of the song is in Cmin. Additionally, the keygram identifies the chorus section (around 50 seconds) as being in Ebmaj, which is consistent with my own perception of the song.


### Tempogram of "Home"


```{r}
Home_tempo <- get_tidy_audio_analysis("2YonDH4haZzgbo3NjpIalE")

Home_tempo |>
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()

```

***

A tempogram is an image that shows what tempo the song is at a given time. This tempogram seems to be very steady around 92 bpm. This makes a lot of sense, because the song is recorded with a click track that was set to, you guessed it, 92 bpm. At the beginning of the song (only for a few seconds) the tempogram seems to be very ambiguous about the tempo. This makes sense, because there is only an audio of voices playing at that time. Only at the end of the song, the music goes off-click and slows down. You can see that the tempo decreases a bit, probably from around 92 to around 86 bpm. 


Comparing inspiration and "Home" {.storyboard}
=====================================

### Comparing spotify Audio features


```{r}
ding <- ggplot(jahe.long,aes(x=variables,value,fill=variable,  text = variable))+
     geom_bar(stat="identity",position="dodge") + theme_bw() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

ggplotly(ding, tooltip = c("text", "value"))
```

***

The bar graph compares the mean values of parameters for my 101 top songs of 2022, represented by red bars, with those of my own song shown in blue. These parameters are ranked between 0 and 1 by Spotify.

##### acousticness
Although I feel like I listen to a lot of acoustic music, It doesn't appear that Spotify things the same. My own song makes use of acoustic guitar, but is still really rated as being very much not acoustic. I think this makes sense, because it's very hard for Spotify to recognise the timbre of an acoustic guitar. This parameter probably looks more at the presence of a drum beat. 

##### danceability and energy
The mean of my top listened songs in 2022 and my own are for these parameters very close. For danceability they are both around 0.6 and for energy both around 0.5. 

##### instrumentalness and speechiness
From this histogram, it seems that I like to listen to songs with a lot of vocal in it. My own song actually seems to be a lot more instrumental. This is in fact true, there is only vocals in short parts of the songs. There is also no normal speech at all in my song, hence the low score in this parameter.

##### liveness
The average score for liveness for the songs that I have listened to is actually pretty straight in the middle, while my own song scores rather low.

##### valence
I seem to be listening to songs that on average lean more to happy, but my own song is ranked as more "sad". 


### Comparing timbre features 

```{r}
MeestBeluisterd2022 <-
  get_playlist_audio_features(
    "thesoundsofspotify",
    "0nYReIA8L6ep3yZp00TLwf"
  ) |>
  slice(1:30) |>
  add_audio_analysis()

Home <- 
  get_playlist_audio_features(
    "thesoundsofspotify",
    "2OKDjAF0WFX2OntThq1N6f"
  ) |>
  slice(1:30) |>
  add_audio_analysis()

Home_new <- Home |> mutate(genre = "Home")

MeestBeluisterd2022 <- MeestBeluisterd2022 |> mutate(genre = "MeestBeluisterd2022")

NummerEnBeluisterd <-
  MeestBeluisterd2022 |>
  mutate(genre = "MeestBeluisterd2022") |>
  bind_rows(Home |> mutate(genre = "Home"))

q <- MeestBeluisterd2022 |>
  mutate(
    timbre =
      map(
        segments,
        compmus_summarise,
        timbre,
        method = "mean"
      )
  ) |>
  select(genre, timbre) |>
  compmus_gather_timbre() |>
  filter(basis %in% c("c01", "c02", "c03", "c04", "c05", "c06")) |> 
  ggplot(aes(x = basis, y = value, fill = "peachpuff")) +
  geom_violin(fill = "peachpuff") +
  scale_fill_viridis_d() +
  labs(x = "Spotify Timbre Coefficients", y = "")


# Add the Home data as points on top of the violins
dingetjedong <- q + geom_point(data = Home_new %>%
                 mutate(
                   timbre = map(
                     segments,
                     compmus_summarise,
                     timbre,
                     method = "mean"
                   )
                 ) %>%
                 select(genre, timbre) %>%
                 compmus_gather_timbre() %>%
                 filter(basis %in% c("c01", "c02", "c03", "c04", "c05", "c06")),
               aes(x = basis, y = value, fill = "peachpuff4"),
               size = 2)

dingetjedongetje <- dingetjedong + 
  guides(fill = FALSE) + theme_bw()

ggplotly(dingetjedongetje, tooltip = c("value"))

```


***

This plot displays the distribution of the first six timbre coefficients extracted by Spotify, depicted as orange violins. Each violin has a purple dot on top, which indicates my song's score for that specific timbre coefficient. I can still understand what the first three coefficients say about the music, so I will look into those three. 

##### c01
This timbre coefficient represents loudness. My most listened songs aren't very spread in loudness, and my song seems to be very much in the middle.

##### c02
This layer is more associated with the lower frequencies; the bass. The songs that I have listened to seem to be very spread when it comes to the bass, but in my song the lower frequencies are very present.

##### c03
In this timbre coefficient you can see the mids. My song scores pretty low compared to my most listened songs. 



### Comparing Acousticness, Energy, Loudness and Mode

```{r}

Home_for_scatterplot <-  get_playlist_audio_features("", "2OKDjAF0WFX2OntThq1N6f")
Home_for_scatterplot$mode <- factor(Home_for_scatterplot$mode)
levels(Home_for_scatterplot$mode) <- c("major", "minor")

my_portfolio3 <- rbind(my_portfolio2, Home_for_scatterplot)


# Create a scatterplot
first_scatter2 <- ggplot(my_portfolio3, aes(x=energy, y=acousticness, size=loudness, text=track.name, color = mode)) +
  geom_point(alpha=0.7) +

  # Add titles and labels to axes
  ggtitle("Playlist Analysis") +
  xlab("Energy") +
  ylab("Acousticness") +
  
  # Customize colors and theme
  scale_color_brewer(palette = "Set1") +
  theme_bw() + guides(size = guide_legend(title=NULL), shape = guide_legend(title=NULL)) + annotate("text", x = 0.40, y = 0.02, label = "Home", color = "skyblue4", size=3)


# Convert the scatterplot to a plotly object and display
ggplotly(first_scatter2, tooltip = c("text"), hoverinfo = "text") 

```

***

I was wondering where my own song would be at the scatter plot I made to show the relation between acoutsicness, energy, mode and loudness. For almost all of these, my song is very close to the song **"Twinkle"** by artist **"Heather"**. My song seems to score quite high on energy, while scoring very low at acoustic. This is very interesting to me. 


### Comparing loudness and tempo, using Kmeans

```{r}

set.seed(123)

my_portfolio3_kmeans <- my_portfolio3 %>% select(tempo, loudness)

my_portfolio3_kmeans <- my_portfolio3_kmeans %>%
  mutate(tempo_rescaled = scale(tempo, center = min(tempo), scale = diff(range(tempo))))

my_portfolio3_kmeans <- my_portfolio3_kmeans %>%
  mutate(loudness_rescaled = scale(loudness, center = min(loudness), scale = diff(range(loudness))))

my_portfolio3_kmeans <- my_portfolio3_kmeans %>% select(tempo_rescaled, loudness_rescaled)



# making sure the names of the artists appear
flippedup <- my_portfolio3 %>% select(track.artists)
names_list <- map(flippedup$track.artists, ~ .x$name)

# making sure the names of the songs appear
#dippedup < my_portfolio2 %>% select(track.name)

# Perform k-means clustering with k = 3 clusters
kmeans_result <- kmeans(my_portfolio3_kmeans, centers = 3)
my_portfolio3_clusters <- data.frame(my_portfolio3_kmeans, cluster = kmeans_result$cluster)

my_portfolio3_clusters$names <- names_list


# Print the cluster assignments
testjebro <- ggplot(my_portfolio3_clusters, aes(x = tempo_rescaled, y = loudness_rescaled, color = factor(cluster), text=names, size = 2)) +
  geom_point(aes(shape = ifelse(names == "Loving", "triangle", "circle"), size = ifelse(names == "Moving", 1.01, 1))) +
  labs(x = "tempo", y = "Loudness", color = "Cluster") + theme_bw() + guides(shape=FALSE, size=FALSE)


ggplotly(testjebro, tooltip = c("text"))

```

***

I was also interested where my own song would be at the scatter plot I made to show the relation between loudness and tempo. My song is the big dot in this plot. It's been assigned to the blue cluster and it's very close to songs by **"Hether"**, **"Andy Shauf"**, and **"Moonchild"** while looking at these two parameters. 


### What song is most similar?

```{r}
Home_song <- get_track_audio_features("2YonDH4haZzgbo3NjpIalE")
Home_song <- Home_song %>% select(danceability, energy, loudness, speechiness, acousticness, instrumentalness, liveness, valence, tempo)

Home_song <- Home_song %>%
  mutate(tempo_rescaled = scale(tempo),
         loudness_rescaled = scale(loudness))

# Home_song <- Home_song %>% select(-tempo, -loudness)
Home_tempo <- 0.220595
Home_loudness <- 0.549770

Home_song[1, "tempo_rescaled"] <- Home_tempo
Home_song[1, "loudness_rescaled"] <- Home_loudness

Home_song <- Home_song %>% select(-tempo, -loudness)

Comparison_listened <- my_portfolio2 %>% select(danceability, energy, loudness, speechiness, acousticness, instrumentalness, liveness, valence, tempo)

Comparison_listened <- Comparison_listened %>%
  mutate(tempo_rescaled = (tempo - min(tempo)) / (max(tempo) - min(tempo)),
         loudness_rescaled = (loudness - min(loudness)) / (max(loudness) - min(loudness)))


Comparison_listened <- Comparison_listened %>% select(-tempo, -loudness)




# print(Home_song)
# print(Comparison_listened)


# Compute the Euclidean distance between Home_song and each song in Comparison_listened
diff <- t(apply(Comparison_listened, 1, function(x) x - Home_song))

# Convert the result to a dataframe
diff_df <- as.data.frame(diff)

sommetje <- diff_df %>% summarise_all(.funs = list(~sum(unlist(.))))

sommetje <- abs(sommetje - 0)

smallest <- min(sommetje[1,])
closest_song_index <- which.min(sommetje[1,])

# my_portfolio2$track.name[closest_song_index]




```

Now finally I wanted to find out what song, according to the Spotify audio features, was the most similar to my own. To do this I followed these steps:

- **1** Select all the numeric values. These were danceability, energy, loudness, speechiness, acousticness, instrumentalness, liveness, valence and tempo.

- **2** Make sure that loudness and tempo were rescaled, so that all values were between 0 and 1. 

- **3** Subtract the parameters of my song from all 101 songs that I have listened to the most. 

- **4** Sum all these values together per row, to get the total difference of the song compared to my song. 

- **5** Find the minimum value and see what song that belangs to. 

After all these steps I found that the song "Come on lets go" by "Broadcast" was the most similar. The total difference of all parameters was only **0.00677**. Below you can listen to the song and decide for yoursel if they are similar. 

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/2omiaJwimfJMlcM6yw2YmA?utm_source=generator" width="100%" height="152" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/2YonDH4haZzgbo3NjpIalE?utm_source=generator" width="100%" height="152" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>




Conclusion
=====================================

Conclusion {data-width=1000}
--------------------------------------------------

### Conclusion

The goal of this portfolio was to see if my song is similar to the songs I have listened to the most in 2022, according to Spotify's audio features. 

Initially I took a look at those songs that could have been an inspiration to me. I found a few different things. First a trend between the acousticness and the energy of the songs and that I seemed to prefer songs that are in a major key. After that I discoverd that I didn't really have a preference for tempo, although the bpm that I listened to the most was 124. I also looked at the relationship between loudness and tempo. 

After that I looked into my own song. I compared the chromagram, cepstogram, self-similarity matrices, chordogram, keygram, and tempogram that I made of my song with my own knowledge of "Home". In many cases they were quite accurate. 

After comparing the songs in my corpus to my own song, I discovered that some of the audio features rated by Spotify between 0 and 1 were very similar to the average of my corpus, such as danceability and energy. However, others were quite different, such as acousticness. In terms of timbre coefficients, my song seemed to contain more bass than most of the songs in my corpus. Lastly, I found that the song that was closest to my own song in all numeric parameters was "Come on lets go" by "Broadcast". If there's a song that I ripped off the most, it's probably that one.



<!-- New tab (1) -->
<!-- ===================================== -->


<!-- ```{r} -->
<!-- # circshift <- function(v, n) { -->
<!-- #   if (n == 0) v else c(tail(v, n), head(v, -n)) -->
<!-- # } -->
<!-- # -->
<!-- # #      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B -->
<!-- # major_chord <- -->
<!-- #   c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0) -->
<!-- # minor_chord <- -->
<!-- #   c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0) -->
<!-- # seventh_chord <- -->
<!-- #   c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0) -->
<!-- # -->
<!-- # major_key <- -->
<!-- #   c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88) -->
<!-- # minor_key <- -->
<!-- #   c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17) -->
<!-- # -->
<!-- # chord_templates <- -->
<!-- #   tribble( -->
<!-- #     ~name, ~template, -->
<!-- #     "Gb:7", circshift(seventh_chord, 6), -->
<!-- #     "Gb:maj", circshift(major_chord, 6), -->
<!-- #     "Bb:min", circshift(minor_chord, 10), -->
<!-- #     "Db:maj", circshift(major_chord, 1), -->
<!-- #     "F:min", circshift(minor_chord, 5), -->
<!-- #     "Ab:7", circshift(seventh_chord, 8), -->
<!-- #     "Ab:maj", circshift(major_chord, 8), -->
<!-- #     "C:min", circshift(minor_chord, 0), -->
<!-- #     "Eb:7", circshift(seventh_chord, 3), -->
<!-- #     "Eb:maj", circshift(major_chord, 3), -->
<!-- #     "G:min", circshift(minor_chord, 7), -->
<!-- #     "Bb:7", circshift(seventh_chord, 10), -->
<!-- #     "Bb:maj", circshift(major_chord, 10), -->
<!-- #     "D:min", circshift(minor_chord, 2), -->
<!-- #     "F:7", circshift(seventh_chord, 5), -->
<!-- #     "F:maj", circshift(major_chord, 5), -->
<!-- #     "A:min", circshift(minor_chord, 9), -->
<!-- #     "C:7", circshift(seventh_chord, 0), -->
<!-- #     "C:maj", circshift(major_chord, 0), -->
<!-- #     "E:min", circshift(minor_chord, 4), -->
<!-- #     "G:7", circshift(seventh_chord, 7), -->
<!-- #     "G:maj", circshift(major_chord, 7), -->
<!-- #     "B:min", circshift(minor_chord, 11), -->
<!-- #     "D:7", circshift(seventh_chord, 2), -->
<!-- #     "D:maj", circshift(major_chord, 2), -->
<!-- #     "F#:min", circshift(minor_chord, 6), -->
<!-- #     "A:7", circshift(seventh_chord, 9), -->
<!-- #     "A:maj", circshift(major_chord, 9), -->
<!-- #     "C#:min", circshift(minor_chord, 1), -->
<!-- #     "E:7", circshift(seventh_chord, 4), -->
<!-- #     "E:maj", circshift(major_chord, 4), -->
<!-- #     "G#:min", circshift(minor_chord, 8), -->
<!-- #     "B:7", circshift(seventh_chord, 11), -->
<!-- #     "B:maj", circshift(major_chord, 11), -->
<!-- #     "D#:min", circshift(minor_chord, 3) -->
<!-- #   ) -->
<!-- # -->
<!-- # key_templates <- -->
<!-- #   tribble( -->
<!-- #     ~name, ~template, -->
<!-- #     "Gb:maj", circshift(major_key, 6), -->
<!-- #     "Bb:min", circshift(minor_key, 10), -->
<!-- #     "Db:maj", circshift(major_key, 1), -->
<!-- #     "F:min", circshift(minor_key, 5), -->
<!-- #     "Ab:maj", circshift(major_key, 8), -->
<!-- #     "C:min", circshift(minor_key, 0), -->
<!-- #     "Eb:maj", circshift(major_key, 3), -->
<!-- #     "G:min", circshift(minor_key, 7), -->
<!-- #     "Bb:maj", circshift(major_key, 10), -->
<!-- #     "D:min", circshift(minor_key, 2), -->
<!-- #     "F:maj", circshift(major_key, 5), -->
<!-- #     "A:min", circshift(minor_key, 9), -->
<!-- #     "C:maj", circshift(major_key, 0), -->
<!-- #     "E:min", circshift(minor_key, 4), -->
<!-- #     "G:maj", circshift(major_key, 7), -->
<!-- #     "B:min", circshift(minor_key, 11), -->
<!-- #     "D:maj", circshift(major_key, 2), -->
<!-- #     "F#:min", circshift(minor_key, 6), -->
<!-- #     "A:maj", circshift(major_key, 9), -->
<!-- #     "C#:min", circshift(minor_key, 1), -->
<!-- #     "E:maj", circshift(major_key, 4), -->
<!-- #     "G#:min", circshift(minor_key, 8), -->
<!-- #     "B:maj", circshift(major_key, 11), -->
<!-- #     "D#:min", circshift(minor_key, 3) -->
<!-- #   ) -->
<!-- # -->
<!-- # -->
<!-- # Home_keygram <- -->
<!-- #   get_tidy_audio_analysis("2YonDH4haZzgbo3NjpIalE") |> -->
<!-- #   compmus_align(beats, segments) |> -->
<!-- #   select(beats) |> -->
<!-- #   unnest(beats) |> -->
<!-- #   mutate( -->
<!-- #     pitches = -->
<!-- #       map(segments, -->
<!-- #         compmus_summarise, pitches, -->
<!-- #         method = "mean", norm = "manhattan" -->
<!-- #       ) -->
<!-- #   ) -->
<!-- # -->
<!-- # Home_keygram2 <- -->
<!-- #   get_tidy_audio_analysis("2YonDH4haZzgbo3NjpIalE") |> -->
<!-- #   compmus_align(sections, segments) |> -->
<!-- #   select(sections) |> -->
<!-- #   unnest(sections) |> -->
<!-- #   mutate( -->
<!-- #     pitches = -->
<!-- #       map(segments, -->
<!-- #         compmus_summarise, pitches, -->
<!-- #         method = "mean", norm = "manhattan" -->
<!-- #       ) -->
<!-- #   ) -->


<!-- ``` -->


<!-- Overview {data-width=500} -->
<!-- -------------------------------------------------- -->

<!-- ### Overview -->

<!-- A chordogram is a visual representation within music analysis. It tries to capture the harmonies within a piece and sets it out in time. -->

<!-- On the right side of this tab, you can see a chordogram and a keygram, both of my piece. The first one is divided into beats, the second divided into sections. -->

<!-- For me, it looks like there's a bit of a struggle making these chordograms for this particular song. The song verse of the song makes use of the following chords, switching every two beats: -->

<!-- **Gmaj** - **Cmin** - **Abmaj** - **C#min** - **Cmin** - **Abmaj** - **Cmin** - **Cmin** -->

<!-- This pattern happens twice in a verse. When looking at the chordogram, it doesn't fully seem to pick up on these chords. However, it gets really close and sometimes it indeed get's it right. **C#min** for example, get's recogised every single time it occurs in the song. This is really interesting to me. -->







<!-- column2 {data-width=500} -->
<!-- -------------------------------------------------- -->

<!-- ### Home chordogram bars -->

<!-- ```{r} -->
<!-- Home_keygram |> -->
<!--   compmus_match_pitch_template( -->
<!--     chord_templates,         # Change to chord_templates if descired -->
<!--     method = "euclidean",  # Try different distance metrics -->
<!--     norm = "manhattan"     # Try different norms -->
<!--   ) |> -->
<!--   ggplot( -->
<!--     aes(x = start + duration / 2, width = duration, y = name, fill = d) -->
<!--   ) + -->
<!--   geom_tile() + -->
<!--   scale_fill_viridis_c(guide = "none") + -->
<!--   theme_minimal() + -->
<!--   labs(x = "Time (s)", y = "") -->
<!-- ``` -->



<!-- ### Home keygram sections -->

<!-- ```{r} -->
<!-- Home_keygram2 |> -->
<!--   compmus_match_pitch_template( -->
<!--     chord_templates,         # Change to chord_templates if descired -->
<!--     method = "euclidean",  # Try different distance metrics -->
<!--     norm = "manhattan"     # Try different norms -->
<!--   ) |> -->
<!--   ggplot( -->
<!--     aes(x = start + duration / 2, width = duration, y = name, fill = d) -->
<!--   ) + -->
<!--   geom_tile() + -->
<!--   scale_fill_viridis_c(guide = "none") + -->
<!--   theme_minimal() + -->
<!--   labs(x = "Time (s)", y = "") -->
<!-- ``` -->

<!-- New tab (2) -->
<!-- ===================================== -->


<!-- column_hist {data-width=750} -->
<!-- -------------------------------------------------- -->

<!-- ### histogram -->
<!-- ```{r} -->
<!-- ggplot(data = my_portfolio_key, aes(x = key)) + geom_histogram(binwidth = 1, fill = "skyblue", color = "black", alpha = 1, ) + scale_x_continuous(breaks = unique(my_portfolio_key$key), labels = unique(my_portfolio_key$key)) + labs(title = "Histogram Key", x = "Key", y = "Frequency") -->



<!-- ``` -->



<!-- column_hist2 {data-width=250} -->
<!-- -------------------------------------------------- -->


<!-- ### Text -->

<!-- On the left side you can see a histogram of the key of all my most listened songs of 2022 -->




<!-- New tab (3) -->
<!-- ===================================== -->


<!-- column_test {data-width=750} -->
<!-- -------------------------------------------------- -->

<!-- ### Nice Plot -->
<!-- ```{r} -->
<!-- # MeestBeluisterd2022 <- -->
<!-- #   get_playlist_audio_features( -->
<!-- #     "thesoundsofspotify", -->
<!-- #     "0nYReIA8L6ep3yZp00TLwf" -->
<!-- #   ) |> -->
<!-- #   slice(1:30) |> -->
<!-- #   add_audio_analysis() -->
<!-- # -->
<!-- # Home <- -->
<!-- #   get_playlist_audio_features( -->
<!-- #     "thesoundsofspotify", -->
<!-- #     "2OKDjAF0WFX2OntThq1N6f" -->
<!-- #   ) |> -->
<!-- #   slice(1:30) |> -->
<!-- #   add_audio_analysis() -->
<!-- # -->
<!-- # Home_new <- Home |> mutate(genre = "Home") -->
<!-- # -->
<!-- # MeestBeluisterd2022 <- MeestBeluisterd2022 |> mutate(genre = "MeestBeluisterd2022") -->
<!-- # -->
<!-- # NummerEnBeluisterd <- -->
<!-- #   MeestBeluisterd2022 |> -->
<!-- #   mutate(genre = "MeestBeluisterd2022") |> -->
<!-- #   bind_rows(Home |> mutate(genre = "Home")) -->
<!-- # -->
<!-- # q <- MeestBeluisterd2022 |> -->
<!-- #   mutate( -->
<!-- #     timbre = -->
<!-- #       map( -->
<!-- #         segments, -->
<!-- #         compmus_summarise, -->
<!-- #         timbre, -->
<!-- #         method = "mean" -->
<!-- #       ) -->
<!-- #   ) |> -->
<!-- #   select(genre, timbre) |> -->
<!-- #   compmus_gather_timbre() |> -->
<!-- #   filter(basis %in% c("c01", "c02", "c03", "c04", "c05", "c06")) |> -->
<!-- #   ggplot(aes(x = basis, y = value, fill = "peachpuff")) + -->
<!-- #   geom_violin(fill = "peachpuff") + -->
<!-- #   scale_fill_viridis_d() + -->
<!-- #   labs(x = "Spotify Timbre Coefficients", y = "") -->
<!-- # -->
<!-- # -->
<!-- # # Add the Home data as points on top of the violins -->
<!-- # dingetjedong <- q + geom_point(data = Home_new %>% -->
<!-- #                  mutate( -->
<!-- #                    timbre = map( -->
<!-- #                      segments, -->
<!-- #                      compmus_summarise, -->
<!-- #                      timbre, -->
<!-- #                      method = "mean" -->
<!-- #                    ) -->
<!-- #                  ) %>% -->
<!-- #                  select(genre, timbre) %>% -->
<!-- #                  compmus_gather_timbre() %>% -->
<!-- #                  filter(basis %in% c("c01", "c02", "c03", "c04", "c05", "c06")), -->
<!-- #                aes(x = basis, y = value, fill = "peachpuff4"), -->
<!-- #                size = 2) -->
<!-- # -->
<!-- # dingetjedongetje <- dingetjedong + -->
<!-- #   guides(fill = FALSE) + theme_bw() -->
<!-- # -->
<!-- # ggplotly(dingetjedongetje, tooltip = c("value")) -->

<!-- ``` -->



<!-- column_test2 {data-width=250} -->
<!-- -------------------------------------------------- -->


<!-- ### Tekst -->
<!-- On the left, you can see violin plots of the timbre of all the songs in my most listened playlist of 2022, compared to my own song. They are shown for the first 6 layers of the timbre computing functions of Spotify. -->





<!-- Old tabs {.storyboard} -->
<!-- ===================================== -->

<!-- ### Chromatogram -->

<!-- #### The Chromagram and cepstogram of my piece: -->

<!-- ```{r} -->
<!-- Home_tidy <- -->
<!--   get_tidy_audio_analysis("2YonDH4haZzgbo3NjpIalE") |> -->
<!--   select(segments) |> -->
<!--   unnest(segments) |> -->
<!--   select(start, duration, pitches) -->

<!-- Home_tidy |> -->
<!--   mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |> -->
<!--   compmus_gather_chroma() |> -->
<!--   ggplot( -->
<!--     aes( -->
<!--       x = start + duration / 2, -->
<!--       width = duration, -->
<!--       y = pitch_class, -->
<!--       fill = value -->
<!--     ) -->
<!--   ) + -->
<!--   geom_tile() + -->
<!--   labs(x = "Time (s)", y = NULL, fill = "Magnitude") + -->
<!--   theme_minimal() + -->
<!--   scale_fill_viridis_c() -->

<!-- Home_Cepstogram <- -->
<!--   get_tidy_audio_analysis("2YonDH4haZzgbo3NjpIalE") |> # Change URI. -->
<!--   compmus_align(bars, segments) |>                     # Change `bars` -->
<!--   select(bars) |>                                      #   in all three -->
<!--   unnest(bars) |>                                      #   of these lines. -->
<!--   mutate( -->
<!--     pitches = -->
<!--       map(segments, -->
<!--         compmus_summarise, pitches, -->
<!--         method = "rms", norm = "euclidean"              # Change summary & norm. -->
<!--       ) -->
<!--   ) |> -->
<!--   mutate( -->
<!--     timbre = -->
<!--       map(segments, -->
<!--         compmus_summarise, timbre, -->
<!--         method = "rms", norm = "euclidean"              # Change summary & norm. -->
<!--       ) -->
<!--   ) -->

<!-- Home_Cepstogram |> -->
<!--   compmus_gather_timbre() |> -->
<!--   ggplot( -->
<!--     aes( -->
<!--       x = start + duration / 2, -->
<!--       width = duration, -->
<!--       y = basis, -->
<!--       fill = value -->
<!--     ) -->
<!--   ) + -->
<!--   geom_tile() + -->
<!--   labs(x = "Time (s)", y = NULL, fill = "Magnitude") + -->
<!--   scale_fill_viridis_c() + -->
<!--   theme_classic() -->


<!-- ``` -->

<!-- ------------------------------------------------------------------------ -->

<!-- On the **left side** you can see the chromagram of my song "Home". Overall it looks pretty clear to distinguish the different pitch classes that are present in the piece. Although I would say the piece is not in one particular key, the verses are in Cminor. The first chorus starts around 50 seconds into the song, and there you can see the C is no longer prevalent in the song. In the outro of the song, starting around 120 seconds in, the song rests on the Cminor chord for a long time and that can be clearly extracted from this chromatogram. -->

<!-- Then on the **right side** a cepstogram of my song can be seen. I found that by using a "euclidean" normalisation  and the "root mean square" as a summary statistic, I got the clearest representation. There are three things that to me are interesting to see in this cepstogram. -->

<!-- - In the first layer c01, which is basically the loudness of the piece at a given time, you can see that on the far left- and right side, the song is very quiet. You can recognise this by the darkness of the blocks. -->
<!-- - In the second layer c02, which shows the presence of lower frequencies (bass), the sections between 0 and 25 seconds and between 60 and 80 seconds look brighter. I can't really explain it that well, other than that it could be assigned to the fact that the loudness of the bass is there the highest, relative to the rest of the piece. -->
<!-- - In the third layer c03, which expresses the presence of the mids, you can see that between 25 and 50 seconds in this layer gets brighter. I think this is because that's where the vocals come in. -->


<!-- ### Self-similarity matrice (New tab) -->

<!-- #### The Self-similarity matrice of my song: -->

<!-- ```{r} -->
<!-- Home_self_sim <- -->
<!--   get_tidy_audio_analysis("2YonDH4haZzgbo3NjpIalE") |> # Change URI. -->
<!--   compmus_align(bars, segments) |>                     # Change `bars` -->
<!--   select(bars) |>                                      #   in all three -->
<!--   unnest(bars) |>                                      #   of these lines. -->
<!--   mutate( -->
<!--     pitches = -->
<!--       map(segments, -->
<!--         compmus_summarise, pitches, -->
<!--         method = "rms", norm = "euclidean"              # Change summary & norm. -->
<!--       ) -->
<!--   ) |> -->
<!--   mutate( -->
<!--     timbre = -->
<!--       map(segments, -->
<!--         compmus_summarise, timbre, -->
<!--         method = "rms", norm = "euclidean"              # Change summary & norm. -->
<!--       ) -->
<!--   ) -->


<!-- bind_rows( -->
<!--   Home_self_sim |> -->
<!--     compmus_self_similarity(pitches, "euclidean") |> -->
<!--     mutate(d = d / max(d), type = "Chroma"), -->
<!--   Home_self_sim |> -->
<!--     compmus_self_similarity(timbre, "euclidean") |> -->
<!--     mutate(d = d / max(d), type = "Timbre") -->
<!-- ) |> -->
<!--   mutate() |> -->
<!--   ggplot( -->
<!--     aes( -->
<!--       x = xstart + xduration / 2, -->
<!--       width = xduration, -->
<!--       y = ystart + yduration / 2, -->
<!--       height = yduration, -->
<!--       fill = d -->
<!--     ) -->
<!--   ) + -->
<!--   geom_tile() + -->
<!--   coord_fixed() + -->
<!--   facet_wrap(~type) + -->
<!--   scale_fill_viridis_c(guide = "none") + -->
<!--   theme_classic() + -->
<!--   labs(x = "", y = "") -->

<!-- ``` -->

<!-- *** -->
<!-- On this tab, you can see both the chroma-based and timbre-based self-similarity matrices of my song. Again, for both I liked using the "euclidean" and the "root mean square" the most. Now I will describe what I see in both matrices. -->

<!-- ##### **Chroma-based** -->
<!-- So based on the looking at the left matrix, you can generally distinguish three different kind of sections. -->
<!-- - Verses: Between both 0 and 45 seconds and 65 and 110 seconds you can se repeating checkerboard like patterns. These are the verses. They make use of the same chord progressions. -->
<!-- - Choruses: Between 45 and 65 and between 110 and 130 you can see the choruses. Although they don't really show diagonals clearly, the same pattern can be seen in both sections. -->
<!-- - Outro: From around 130 seconds in, the outro starts. This is represented in the top right of the matrix and a clear black checkerboard box can be seen. -->




<!-- ### Introduction -->

<!-- (I just changed my topic 2 days ago, so let me know what you think.) -->

<!-- Just three days ago, the debut single, "Home", of my band was released on Spotify, marking a significant moment in my journey as a songwriter. It's the first time that the public can openly listen to my music, and I'm feeling a mix of excitement and nerves as I await the response from listeners. -->

<!-- But the release of my song on Spotify also offers new possibilities beyond just reaching a wider audience. With the help of the Spotify API, I can analyze my song and gain insights into its musical characteristics. -->

<!-- As a songwriter, I'm always fascinated by the creative process and how other music shapes the songs we write. For this project, I'm curious to explore the ways in which different artists and genres have influenced my own music. -->

<!-- To do this, I plan to compare my debut song to my top songs of 2022 and identify any correlations or similarities. Which artists or songs have had the greatest impact on my writing, and have I unintentionally incorporated elements of other music into my work. In other words: have I maybe accidentally ripped someone off? -->

<!-- As a songwriter, I believe that understanding the creative influences behind our music can enhance our artistry and lead to new discoveries. This project offers a unique opportunity to delve deeper into the musical elements that shape my writing and explore the ways in which other artists and genres have impacted my work. I'm excited to embark on this journey of self-discovery and to share my findings with fellow music enthusiasts. -->

<!-- If you stumble upon this dashboard and you would like to see what the song, "Home", sounds like, here is the Spotify link: <https://open.spotify.com/track/2YonDH4haZzgbo3NjpIalE?si=324e6a2d4e424358> -->

<!-- ### Comparing numerical variables -->

<!-- ```{r} -->
<!-- ding <- ggplot(jahe.long,aes(x=variables,value,fill=variable))+ -->
<!--      geom_bar(stat="identity",position="dodge") + theme(axis.text.x = element_text(angle = 90)) -->

<!-- ggplotly(ding) -->
<!-- ``` -->

<!-- ------------------------------------------------------------------------ -->

<!-- In the bar graph in red you can see the mean for all the variables of my 101 top songs of 2022. These variables are the ones that are ranked between 0 and 1 by Spotify. In blue you see these numbers for my song. They differ the most for accounstichness. -->



