---
title: "My Portfolio"
author: "Anne Leendertse"
date: "Spring 2023"
output:
  flexdashboard::flex_dashboard:
    storyboard: true
    self_contained: false
    theme: 
      version: 4
      bootswatch: minty
---

```{r}
library(tidyverse)
library(spotifyr)
library(plotly)
library(compmus)
library(tidyr)
library(reshape2)
library(shiny)
library(GGally)
library(patchwork)

```

```{r}
andy_shauf <- get_track_audio_features("15WJWyrI3c6aRuvbYgMcKv")
my_portfolio <- get_playlist_audio_features("", "3MxAsxThsrUCjU4QAplkg0")
my_portfolio2 <- get_playlist_audio_features("", "0nYReIA8L6ep3yZp00TLwf")
my_portfolio2$mode <- factor(my_portfolio2$mode)
levels(my_portfolio2$mode) <- c("minor", "major")
Home <- get_track_audio_features("2YonDH4haZzgbo3NjpIalE")

Home_num <- Home %>% select_if(is.numeric) %>% select(-time_signature, -key, -tempo, -duration_ms, -loudness)
andy_shauf_num <- andy_shauf %>% select_if(is.numeric)

my_portfolio2_num <- my_portfolio2 %>% select_if(is.numeric)
my_portfolio2_clean <- my_portfolio2_num %>% select(-track.disc_number, -track.disc_number, -track.popularity, -track.track_number, -track.album.total_tracks, -time_signature, -key, -tempo, -track.duration_ms, -loudness)

my_portfolio_key <- my_portfolio2 %>% select(key)

my_portfolio_tempo <- my_portfolio2 %>% select(tempo, track.name)
my_portfolio_clean_mean <- apply(my_portfolio2_clean, 2, mean) 
compare_influence_writing <- rbind(my_portfolio_clean_mean, Home_num)
compare_influence_writing <- t(compare_influence_writing )


colnames(compare_influence_writing)[2]  <- "Home" 
colnames(compare_influence_writing)[1]  <- "Influence" 

jahe <- data.frame(compare_influence_writing)




jahe.long <- melt(jahe)

namen <- rownames(jahe)
variables <- append(namen, rownames(jahe))


```

Introduction
=====================================

Introduction {data-width=500}
--------------------------------------------------

### Introduction

Just last month, the debut single, "Home", of my band was released on Spotify, marking a significant moment in my journey as a songwriter. It's the first time that the public can openly listen to my music, and I'm feeling a mix of excitement and nerves as I await the response from listeners.

But the release of my song on Spotify also offers new possibilities beyond just reaching a wider audience. With the help of the Spotify API, I can analyze my song and gain insights into its musical characteristics.

As a songwriter, I'm always fascinated by the creative process and how other music shapes the songs we write. For this project, I'm curious to explore the ways in which different artists and genres have influenced my own music.

To do this, I plan to compare my debut song to my top songs of 2022 and identify any correlations or similarities. Which artists or songs have had the greatest impact on my writing, and have I unintentionally incorporated elements of other music into my work. In other words: have I maybe accidentally ripped someone off?

As a songwriter, I believe that understanding the creative influences behind our music can enhance our artistry and lead to new discoveries. This project offers a unique opportunity to delve deeper into the musical elements that shape my writing and explore the ways in which other artists and genres have impacted my work. 

Before making a comparison, I want to analyze the songs that I listened to the most in 2022, with the aim of identifying any interesting trends or correlations. This section can be accessed by clicking the "Inspirations" button at the top of the page. Next, I will turn my attention to my own song, which can be found by clicking on the "My song" button. Finally, in the last section, I will seek to make comparisons and identify the song that Spotify suggests is most similar to my own.

If you want to listen to what my song "Home" and the songs that could have influenced me sound like, look on the right side of this tab.


embededspotify {data-width=500}
--------------------------------------------------

### Home {.myclass style="height:100px;"}

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/2YonDH4haZzgbo3NjpIalE?utm_source=generator" width="100%" height="352" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>


### My top songs of 2022


<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/0nYReIA8L6ep3yZp00TLwf?utm_source=generator" width="100%" height="352" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>



Inspiration feature analysis {.storyboard}
=====================================

### Introduction

hier wat zooi ook



### Acousticness, Energy, Loudness and Mode

```{r Acousticness, Energy, Loudness and Mode}
# Create a scatterplot
first_scatter <- ggplot(my_portfolio2, aes(x=energy, y=acousticness, size=loudness, text=track.name, color = mode)) +
  geom_point(alpha=0.7) +

  # Add titles and labels to axes
  ggtitle("Playlist Analysis") +
  xlab("Energy") +
  ylab("Acousticness") +
  
  # Customize colors and theme
  scale_color_brewer(palette = "Set1") +
  theme_minimal() + guides(loudness=FALSE)

lm <- lm(acousticness ~ energy, data = my_portfolio2)
first_scatter <- first_scatter + geom_abline(intercept = lm$coefficients[1], slope = lm$coefficients[2]) + theme_bw() + guides(loudness=FALSE)


# Convert the scatterplot to a plotly object and display
ggplotly(first_scatter, tooltip = c("text"), hoverinfo = "text") 

```

***

This plot displays a scatter plot depicting the relationship between Energy and Acousticness of all songs in my corpus. A downwards trend is visible between the two variables. To show this trend more clearly, I plotted a line following the data. Songs in my corpus that have higher levels of acousticness tend to have lower levels of energy, and vice versa. This seems logical since acoustic songs generally have less energy than non-acoustic songs.

The size of the dots shows the loudness of the song. The louder the song, the bigger the dot is shown in the plot. You can see that acoustic songs also seem to be a bit less loud. I think there are also some interesting cases when it comes to loudness. First of all, in the top left you see a very small dot. This is the song "Quarters" by American indie/folk band "Wilco". This song seems to be the quietest song by far; a real outlier. Secondly, there is a song called "Reelin' in the years" by the band "Steely Dan" (The song sits around 0.55 Energy and 0.20 Acousticness). It seems to be a lot less loud than all of the songs around it. I think this might be due to the fact that this song, unlike most of the songs in this corpus, is produced in the early 70s. Songs that are mastered a long time ago sometimes are a bit less loud. 

The dots in the plot are color-coded to represent the mode of the song, which indicates whether it is in a major or minor key. While there does not appear to be a correlation between mode and the other variables, the plot does reveal that there are more songs in major key than in minor key in my corpus.


### histogram keys
```{r}

note_labels <- c("C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B")

histogram1 <- ggplot(data = my_portfolio_key, aes(x = key)) + geom_histogram(binwidth = 1, fill = "indianred1", color = "indianred4", alpha = 1, ) + scale_x_continuous(breaks = 0:11, labels = note_labels) + labs(title = "Histogram Key", x = "Key", y = "Frequency") + theme_bw()


ggplotly(histogram1)
```


***

Hier iets zeggen bro

### histogram tempo

```{r}
index_max_tempo <- which.max(my_portfolio_tempo$tempo)
track_name_max_tempo <- my_portfolio_tempo$track.name[index_max_tempo]

index_min_tempo <- which.min(my_portfolio_tempo$tempo)
track_name_min_tempo <- my_portfolio_tempo$track.name[index_min_tempo]



histogram1 <- ggplot(data = my_portfolio_tempo, aes(x = tempo)) + geom_histogram(binwidth = 2, fill = "steelblue1", color = "steelblue4", alpha = 1) + labs(title = "Histogram Tempo", x = "Tempo", y = "Frequency") + theme_bw() + annotate("text", x = 124, y = 8, label = "Most frequent tempo", color = "steelblue4") + annotate("text", x = 124, y = 7.5, label = "124bpm", color = "steelblue4") + annotate("text", x = 189, y = 2, label = track_name_max_tempo, size = 2 , color = "steelblue1") + annotate("text", x = 189, y = 2.2, label = "192bpm", size = 2 , color = "steelblue1") + annotate("text", x = 65, y = 3, label = track_name_min_tempo, size = 2 , color = "steelblue1") + annotate("text", x = 64, y = 3.2, label = "64bpm", size = 2 , color = "steelblue1")

ggplotly(histogram1)

```

***

Hier nog wat gooien danmaar


### Kmeans

```{r}
# Assuming your data is stored in a data frame called "music_data"
set.seed(123)  # For reproducibility

my_portfolio2_kmeans <- my_portfolio2 %>% select(tempo, loudness)

my_portfolio2_kmeans <- my_portfolio2_kmeans %>%
  mutate(tempo_rescaled = scale(tempo, center = min(tempo), scale = diff(range(tempo))))

my_portfolio2_kmeans <- my_portfolio2_kmeans %>%
  mutate(loudness_rescaled = scale(loudness, center = min(loudness), scale = diff(range(loudness))))

my_portfolio2_kmeans <- my_portfolio2_kmeans %>% select(tempo_rescaled, loudness_rescaled)



# making sure the names of the artists appear
flippedup <- my_portfolio2 %>% select(track.artists)
names_list <- map(flippedup$track.artists, ~ .x$name)

# making sure the names of the songs appear
#dippedup < my_portfolio2 %>% select(track.name)

# Perform k-means clustering with k = 3 clusters
kmeans_result <- kmeans(my_portfolio2_kmeans, centers = 3)
my_portfolio2_clusters <- data.frame(my_portfolio2_kmeans, cluster = kmeans_result$cluster)

my_portfolio2_clusters$names <- names_list


# Print the cluster assignments
testjebro <- ggplot(my_portfolio2_clusters, aes(x = tempo_rescaled, y = loudness_rescaled, color = factor(cluster), text=names)) +
  geom_point(aes(shape = ifelse(names == "Loving", "triangle", "circle"))) +
  labs(x = "tempo", y = "Loudness", color = "Cluster") + theme_bw() + guides(shape=FALSE)


ggplotly(testjebro, tooltip = c("text"))

```

***

jo gappie hier wat tekst

Audio analysis of "Home" {.storyboard}
=====================================

### Introduction

Hier een intro


### Chromagram

```{r}
Home_tidy <-
  get_tidy_audio_analysis("2YonDH4haZzgbo3NjpIalE") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

Home_tidy |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()
```

***

This is an image of the chromagram for my song "Home". A chromagram displays which pitch classes are present in the song at any given moment. The brightest areas represent the pitch class that is most prominent. For the norm for the chroma vectors, which represent the distribution of musical pitch classes in a song, I've chosen "euclidean". This because it seemed to generate the clearest result. 

Overall it looks pretty clear to distinguish the different pitch classes that are present in the piece. Although I would say the entire piece is not in only one key, the **verses** are in Cminor. Hence the fact that the C pitch class is so clearly represented in this image. The first **chorus** starts around 50 seconds into the song, and there you can see the C is no longer prevalent in the song. In the outro of the song, starting around 120 seconds in, the song rests on the Cminor chord for a long time and that can be clearly extracted from this chromagram. 


### Cepstogram

```{r}
Home_Cepstogram <-
  get_tidy_audio_analysis("2YonDH4haZzgbo3NjpIalE") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

Home_Cepstogram |>
  compmus_gather_timbre() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +                              
  theme_classic()
```

***

Here a cepstogram of my song can be seen. A cepstogram gives information about the timbre features (c01-c12) of a song. I found that by using a "euclidean" normalisation  and the "root mean square" as a summary statistic, I got the clearest representation. There are three things that to me are interesting to see in this cepstogram:

- In the first layer c01, which is basically the loudness of the piece at a given time, you can see that on the far left- and right side, the song is very quiet. You can recognise this by the darkness of the blocks.  
- In the second layer c02, which shows the presence of lower frequencies (bass), the sections between 0 and 25 seconds and between 60 and 80 seconds (verses) look brighter. I think that this might be the case, because the loudness of the bass is there the highest, relative to the rest of the piece. Also, the bass plays some higher notes during the chorusses of the song (first one starts around 50 seconds).
- In the third layer c03, which expresses the presence of the mids, you can see that between 25 and 50 seconds in this layer gets brighter. I think this is because that's where the vocals come in. 



### Self-similarity matrices

```{r}
Home_self_sim <-
  get_tidy_audio_analysis("2YonDH4haZzgbo3NjpIalE") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )


bind_rows(
  Home_self_sim |>
    compmus_self_similarity(pitches, "euclidean") |>
    mutate(d = d / max(d), type = "Chroma"),
  Home_self_sim |>
    compmus_self_similarity(timbre, "euclidean") |>
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |>
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "")

```

***

On this tab, you can see both the chroma-based and timbre-based self-similarity matrices of my song. Self-similarity matrices can show you the structure of a song. Again, for both I found that using the "euclidean", worked best, and the "root mean square" the most. Now I will describe what I see in both matrices.

##### **Chroma-based**
So based on the looking at the left matrix, you can sort of distinguish three different kind of sections. 

- **Verses**: Between both 0 and 45 seconds and 65 and 110 seconds you can se repeating checkerboard like patterns. These are the verses. They make use of the same chord progressions.

- **Choruses**: Between 45 and 65 and between 110 and 130 you can see the choruses. Although they don't really show diagonals clearly, the same pattern can be seen in both sections.

- **Outro**: From around 130 seconds in, the outro starts. This is represented in the top right of the matrix and a clear black checkerboard box can be seen. 

You also can see that the piece repeats itself after the first chorus, because new parallel diagonal lines start from around 65 seconds.  

##### **Timbre-based**
So based on the looking at the right matrix, I think you can distinguish more than three sections. 

I believe that the reason for the difference between this matrix and the chroma-based one lies in the fact that the verses consist of two distinct parts. The verse is the same chord progression repeated 2 times, but the first in the first loop, there are no vocals, but rather a high pitched melody played on a synthesizer. The timbre is therefore very different between the two repetitions of the verse. 


### Chordo- and keygram
```{r}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )


Home_keygram <-
  get_tidy_audio_analysis("2YonDH4haZzgbo3NjpIalE") |>
  compmus_align(beats, segments) |>
  select(beats) |>
  unnest(beats) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

Home_keygram2 <-
  get_tidy_audio_analysis("2YonDH4haZzgbo3NjpIalE") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
```


```{r}
# Home_keygram |>
#   compmus_match_pitch_template(
#     chord_templates,         # Change to chord_templates if descired
#     method = "euclidean",  # Try different distance metrics
#     norm = "manhattan"     # Try different norms
#   ) |>
#   ggplot(
#     aes(x = start + duration / 2, width = duration, y = name, fill = d)
#   ) +
#   geom_tile() +
#   scale_fill_viridis_c(guide = "none") +
#   theme_minimal() +
#   labs(x = "Time (s)", y = "")
# 
# Home_keygram2 |>
#   compmus_match_pitch_template(
#     chord_templates,         # Change to chord_templates if descired
#     method = "euclidean",  # Try different distance metrics
#     norm = "manhattan"     # Try different norms
#   ) |>
#   ggplot(
#     aes(x = start + duration / 2, width = duration, y = name, fill = d)
#   ) +
#   geom_tile() +
#   scale_fill_viridis_c(guide = "none") +
#   theme_minimal() +
#   labs(x = "Time (s)", y = "")


combined_plots <- Home_keygram |>
  compmus_match_pitch_template(
    chord_templates,
    method = "euclidean",
    norm = "manhattan"
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "") +
  plot_layout(ncol = 2, widths = c(5,5)) +
  Home_keygram2 |>
  compmus_match_pitch_template(
    chord_templates,
    method = "euclidean",
    norm = "manhattan"
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")

combined_plots <- combined_plots + plot_layout(ncol = 2, widths = c(50, 50))

combined_plots
```

***

ff checke hoe dit eruit ziet dan he?


### Tempogram


```{r}
# Home_tempo <- get_tidy_audio_analysis("2YonDH4haZzgbo3NjpIalE")
# 
# Home_tempo |>
#   tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) |>
#   ggplot(aes(x = time, y = bpm, fill = power)) +
#   geom_raster() +
#   scale_fill_viridis_c(guide = "none") +
#   labs(x = "Time (s)", y = "Tempo (BPM)") +
#   theme_classic()

```

***

A tempogram is an image that shows what tempo the song is at a given time. This tempogram seems to be very steady around 92 bpm. This makes a lot of sense, because the song is recorded with a click track that was set to, you guessed it, 92 bpm. At the beginning of the song (only for a few seconds) the tempogram seems to be very ambiguous about the tempo. This makes sense, because there is only an audio of voices playing at that time. Only at the end of the song, the music goes off-click and slows down. You can see that the tempo decreases a bit, probably from around 92 to around 86 bpm. 


Comparing inspiration and "Home" {.storyboard}
=====================================

### Spotify Audio features


```{r}
ding <- ggplot(jahe.long,aes(x=variables,value,fill=variable,  text = variable))+
     geom_bar(stat="identity",position="dodge") + theme_bw() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

ggplotly(ding, tooltip = c("text", "value"))
```

***

In the bar graph in red you can see the mean for all the variables of my 101 top songs of 2022. These variables are the ones that are ranked between 0 and 1 by Spotify. In blue you see these numbers for my song. They differ the most for acounsticness. 

### Timbre features 

```{r}
MeestBeluisterd2022 <-
  get_playlist_audio_features(
    "thesoundsofspotify",
    "0nYReIA8L6ep3yZp00TLwf"
  ) |>
  slice(1:30) |>
  add_audio_analysis()

Home <- 
  get_playlist_audio_features(
    "thesoundsofspotify",
    "2OKDjAF0WFX2OntThq1N6f"
  ) |>
  slice(1:30) |>
  add_audio_analysis()

Home_new <- Home |> mutate(genre = "Home")

MeestBeluisterd2022 <- MeestBeluisterd2022 |> mutate(genre = "MeestBeluisterd2022")

NummerEnBeluisterd <-
  MeestBeluisterd2022 |>
  mutate(genre = "MeestBeluisterd2022") |>
  bind_rows(Home |> mutate(genre = "Home"))

q <- MeestBeluisterd2022 |>
  mutate(
    timbre =
      map(
        segments,
        compmus_summarise,
        timbre,
        method = "mean"
      )
  ) |>
  select(genre, timbre) |>
  compmus_gather_timbre() |>
  filter(basis %in% c("c01", "c02", "c03", "c04", "c05", "c06")) |> 
  ggplot(aes(x = basis, y = value, fill = "peachpuff")) +
  geom_violin(fill = "peachpuff") +
  scale_fill_viridis_d() +
  labs(x = "Spotify Timbre Coefficients", y = "")


# Add the Home data as points on top of the violins
dingetjedong <- q + geom_point(data = Home_new %>%
                 mutate(
                   timbre = map(
                     segments,
                     compmus_summarise,
                     timbre,
                     method = "mean"
                   )
                 ) %>%
                 select(genre, timbre) %>%
                 compmus_gather_timbre() %>%
                 filter(basis %in% c("c01", "c02", "c03", "c04", "c05", "c06")),
               aes(x = basis, y = value, fill = "peachpuff4"),
               size = 2)

dingetjedongetje <- dingetjedong + 
  guides(fill = FALSE) + theme_bw()

ggplotly(dingetjedongetje, tooltip = c("value"))

```


***

Hier wat over lullen



### Scatter plot

```{r}

Home_for_scatterplot <-  get_playlist_audio_features("", "2OKDjAF0WFX2OntThq1N6f")
Home_for_scatterplot$mode <- factor(Home_for_scatterplot$mode)
levels(Home_for_scatterplot$mode) <- c("minor", "major")

my_portfolio3 <- rbind(my_portfolio2, Home_for_scatterplot)


# Create a scatterplot
first_scatter2 <- ggplot(my_portfolio3, aes(x=energy, y=acousticness, size=loudness, text=track.name, color = mode)) +
  geom_point(alpha=0.7) +

  # Add titles and labels to axes
  ggtitle("Playlist Analysis") +
  xlab("Energy") +
  ylab("Acousticness") +
  
  # Customize colors and theme
  scale_color_brewer(palette = "Set1") +
  theme_bw() + guides(size = guide_legend(title=NULL), shape = guide_legend(title=NULL)) + annotate("text", x = 0.40, y = 0.02, label = "Home", color = "red", size=3)


# Convert the scatterplot to a plotly object and display
ggplotly(first_scatter2, tooltip = c("text"), hoverinfo = "text") 

```

***

hier nog tekst


### Kmeans

```{r}

set.seed(123)

my_portfolio3_kmeans <- my_portfolio3 %>% select(tempo, loudness)

my_portfolio3_kmeans <- my_portfolio3_kmeans %>%
  mutate(tempo_rescaled = scale(tempo, center = min(tempo), scale = diff(range(tempo))))

my_portfolio3_kmeans <- my_portfolio3_kmeans %>%
  mutate(loudness_rescaled = scale(loudness, center = min(loudness), scale = diff(range(loudness))))

my_portfolio3_kmeans <- my_portfolio3_kmeans %>% select(tempo_rescaled, loudness_rescaled)



# making sure the names of the artists appear
flippedup <- my_portfolio3 %>% select(track.artists)
names_list <- map(flippedup$track.artists, ~ .x$name)

# making sure the names of the songs appear
#dippedup < my_portfolio2 %>% select(track.name)

# Perform k-means clustering with k = 3 clusters
kmeans_result <- kmeans(my_portfolio3_kmeans, centers = 3)
my_portfolio3_clusters <- data.frame(my_portfolio3_kmeans, cluster = kmeans_result$cluster)

my_portfolio3_clusters$names <- names_list


# Print the cluster assignments
testjebro <- ggplot(my_portfolio3_clusters, aes(x = tempo_rescaled, y = loudness_rescaled, color = factor(cluster), text=names)) +
  geom_point(aes(shape = ifelse(names == "Loving", "triangle", "circle"), size = ifelse(names == "Moving", 1.01, 1))) +
  labs(x = "tempo", y = "Loudness", color = "Cluster") + theme_bw() + guides(shape=FALSE, size=FALSE)


ggplotly(testjebro, tooltip = c("text"))

```

***

hier ook nog wat


### Welke pokkoe het meeste?

```{r}
Home_song <- get_track_audio_features("2YonDH4haZzgbo3NjpIalE")
Home_song <- Home_song %>% select(danceability, energy, loudness, speechiness, acousticness, instrumentalness, liveness, valence, tempo)

Home_song <- Home_song %>%
  mutate(tempo_rescaled = scale(tempo),
         loudness_rescaled = scale(loudness))

# Home_song <- Home_song %>% select(-tempo, -loudness)
Home_tempo <- 0.220595
Home_loudness <- 0.549770

Home_song[1, "tempo_rescaled"] <- Home_tempo
Home_song[1, "loudness_rescaled"] <- Home_loudness

Home_song <- Home_song %>% select(-tempo, -loudness)

Comparison_listened <- my_portfolio2 %>% select(danceability, energy, loudness, speechiness, acousticness, instrumentalness, liveness, valence, tempo)

Comparison_listened <- Comparison_listened %>%
  mutate(tempo_rescaled = (tempo - min(tempo)) / (max(tempo) - min(tempo)),
         loudness_rescaled = (loudness - min(loudness)) / (max(loudness) - min(loudness)))


Comparison_listened <- Comparison_listened %>% select(-tempo, -loudness)




# print(Home_song)
# print(Comparison_listened)


# Compute the Euclidean distance between Home_song and each song in Comparison_listened
diff <- t(apply(Comparison_listened, 1, function(x) x - Home_song))

# Convert the result to a dataframe
diff_df <- as.data.frame(diff)

sommetje <- diff_df %>% summarise_all(.funs = list(~sum(unlist(.))))

sommetje <- abs(sommetje - 0)

smallest <- min(sommetje[1,])
closest_song_index <- which.min(sommetje[1,])

# my_portfolio2$track.name[closest_song_index]




```

Now finally I wanted to find out what song, according to the Spotify audio features, was the most similar to my own. To do this I followed these steps:

- **1** Select all the numeric values. These were danceability, energy, loudness, speechiness, acousticness, instrumentalness, liveness, valence and tempo.

- **2** Make sure that loudness and tempo were rescaled, so that all values were between 0 and 1. 

- **3** Subtract the parameters of my song from all 101 songs that I have listened to the most. 

- **4** Sum all these values together per row, to get the total difference of the song compared to my song. 

- **5** Find the minimum value and see what song that belangs to. 

After all these steps I found that the song "Come on lets go" by "Broadcast" was the most similar. The total difference of all parameters was only **0.00677**. Below you can listen to the song and decide for yoursel if they are similar. 

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/2omiaJwimfJMlcM6yw2YmA?utm_source=generator" width="100%" height="152" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/2YonDH4haZzgbo3NjpIalE?utm_source=generator" width="100%" height="152" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>




Conclusion
=====================================

Conclusion {data-width=500}
--------------------------------------------------

### Conclusion

Hiier wat zooi brodda


Conclusion2 {data-width=500}
--------------------------------------------------

### Photo of something


<!-- New tab (1) -->
<!-- ===================================== -->


<!-- ```{r} -->
<!-- # circshift <- function(v, n) { -->
<!-- #   if (n == 0) v else c(tail(v, n), head(v, -n)) -->
<!-- # } -->
<!-- # -->
<!-- # #      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B -->
<!-- # major_chord <- -->
<!-- #   c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0) -->
<!-- # minor_chord <- -->
<!-- #   c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0) -->
<!-- # seventh_chord <- -->
<!-- #   c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0) -->
<!-- # -->
<!-- # major_key <- -->
<!-- #   c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88) -->
<!-- # minor_key <- -->
<!-- #   c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17) -->
<!-- # -->
<!-- # chord_templates <- -->
<!-- #   tribble( -->
<!-- #     ~name, ~template, -->
<!-- #     "Gb:7", circshift(seventh_chord, 6), -->
<!-- #     "Gb:maj", circshift(major_chord, 6), -->
<!-- #     "Bb:min", circshift(minor_chord, 10), -->
<!-- #     "Db:maj", circshift(major_chord, 1), -->
<!-- #     "F:min", circshift(minor_chord, 5), -->
<!-- #     "Ab:7", circshift(seventh_chord, 8), -->
<!-- #     "Ab:maj", circshift(major_chord, 8), -->
<!-- #     "C:min", circshift(minor_chord, 0), -->
<!-- #     "Eb:7", circshift(seventh_chord, 3), -->
<!-- #     "Eb:maj", circshift(major_chord, 3), -->
<!-- #     "G:min", circshift(minor_chord, 7), -->
<!-- #     "Bb:7", circshift(seventh_chord, 10), -->
<!-- #     "Bb:maj", circshift(major_chord, 10), -->
<!-- #     "D:min", circshift(minor_chord, 2), -->
<!-- #     "F:7", circshift(seventh_chord, 5), -->
<!-- #     "F:maj", circshift(major_chord, 5), -->
<!-- #     "A:min", circshift(minor_chord, 9), -->
<!-- #     "C:7", circshift(seventh_chord, 0), -->
<!-- #     "C:maj", circshift(major_chord, 0), -->
<!-- #     "E:min", circshift(minor_chord, 4), -->
<!-- #     "G:7", circshift(seventh_chord, 7), -->
<!-- #     "G:maj", circshift(major_chord, 7), -->
<!-- #     "B:min", circshift(minor_chord, 11), -->
<!-- #     "D:7", circshift(seventh_chord, 2), -->
<!-- #     "D:maj", circshift(major_chord, 2), -->
<!-- #     "F#:min", circshift(minor_chord, 6), -->
<!-- #     "A:7", circshift(seventh_chord, 9), -->
<!-- #     "A:maj", circshift(major_chord, 9), -->
<!-- #     "C#:min", circshift(minor_chord, 1), -->
<!-- #     "E:7", circshift(seventh_chord, 4), -->
<!-- #     "E:maj", circshift(major_chord, 4), -->
<!-- #     "G#:min", circshift(minor_chord, 8), -->
<!-- #     "B:7", circshift(seventh_chord, 11), -->
<!-- #     "B:maj", circshift(major_chord, 11), -->
<!-- #     "D#:min", circshift(minor_chord, 3) -->
<!-- #   ) -->
<!-- # -->
<!-- # key_templates <- -->
<!-- #   tribble( -->
<!-- #     ~name, ~template, -->
<!-- #     "Gb:maj", circshift(major_key, 6), -->
<!-- #     "Bb:min", circshift(minor_key, 10), -->
<!-- #     "Db:maj", circshift(major_key, 1), -->
<!-- #     "F:min", circshift(minor_key, 5), -->
<!-- #     "Ab:maj", circshift(major_key, 8), -->
<!-- #     "C:min", circshift(minor_key, 0), -->
<!-- #     "Eb:maj", circshift(major_key, 3), -->
<!-- #     "G:min", circshift(minor_key, 7), -->
<!-- #     "Bb:maj", circshift(major_key, 10), -->
<!-- #     "D:min", circshift(minor_key, 2), -->
<!-- #     "F:maj", circshift(major_key, 5), -->
<!-- #     "A:min", circshift(minor_key, 9), -->
<!-- #     "C:maj", circshift(major_key, 0), -->
<!-- #     "E:min", circshift(minor_key, 4), -->
<!-- #     "G:maj", circshift(major_key, 7), -->
<!-- #     "B:min", circshift(minor_key, 11), -->
<!-- #     "D:maj", circshift(major_key, 2), -->
<!-- #     "F#:min", circshift(minor_key, 6), -->
<!-- #     "A:maj", circshift(major_key, 9), -->
<!-- #     "C#:min", circshift(minor_key, 1), -->
<!-- #     "E:maj", circshift(major_key, 4), -->
<!-- #     "G#:min", circshift(minor_key, 8), -->
<!-- #     "B:maj", circshift(major_key, 11), -->
<!-- #     "D#:min", circshift(minor_key, 3) -->
<!-- #   ) -->
<!-- # -->
<!-- # -->
<!-- # Home_keygram <- -->
<!-- #   get_tidy_audio_analysis("2YonDH4haZzgbo3NjpIalE") |> -->
<!-- #   compmus_align(beats, segments) |> -->
<!-- #   select(beats) |> -->
<!-- #   unnest(beats) |> -->
<!-- #   mutate( -->
<!-- #     pitches = -->
<!-- #       map(segments, -->
<!-- #         compmus_summarise, pitches, -->
<!-- #         method = "mean", norm = "manhattan" -->
<!-- #       ) -->
<!-- #   ) -->
<!-- # -->
<!-- # Home_keygram2 <- -->
<!-- #   get_tidy_audio_analysis("2YonDH4haZzgbo3NjpIalE") |> -->
<!-- #   compmus_align(sections, segments) |> -->
<!-- #   select(sections) |> -->
<!-- #   unnest(sections) |> -->
<!-- #   mutate( -->
<!-- #     pitches = -->
<!-- #       map(segments, -->
<!-- #         compmus_summarise, pitches, -->
<!-- #         method = "mean", norm = "manhattan" -->
<!-- #       ) -->
<!-- #   ) -->


<!-- ``` -->


<!-- Overview {data-width=500} -->
<!-- -------------------------------------------------- -->

<!-- ### Overview -->

<!-- A chordogram is a visual representation within music analysis. It tries to capture the harmonies within a piece and sets it out in time. -->

<!-- On the right side of this tab, you can see a chordogram and a keygram, both of my piece. The first one is divided into beats, the second divided into sections. -->

<!-- For me, it looks like there's a bit of a struggle making these chordograms for this particular song. The song verse of the song makes use of the following chords, switching every two beats: -->

<!-- **Gmaj** - **Cmin** - **Abmaj** - **C#min** - **Cmin** - **Abmaj** - **Cmin** - **Cmin** -->

<!-- This pattern happens twice in a verse. When looking at the chordogram, it doesn't fully seem to pick up on these chords. However, it gets really close and sometimes it indeed get's it right. **C#min** for example, get's recogised every single time it occurs in the song. This is really interesting to me. -->







<!-- column2 {data-width=500} -->
<!-- -------------------------------------------------- -->

<!-- ### Home chordogram bars -->

<!-- ```{r} -->
<!-- Home_keygram |> -->
<!--   compmus_match_pitch_template( -->
<!--     chord_templates,         # Change to chord_templates if descired -->
<!--     method = "euclidean",  # Try different distance metrics -->
<!--     norm = "manhattan"     # Try different norms -->
<!--   ) |> -->
<!--   ggplot( -->
<!--     aes(x = start + duration / 2, width = duration, y = name, fill = d) -->
<!--   ) + -->
<!--   geom_tile() + -->
<!--   scale_fill_viridis_c(guide = "none") + -->
<!--   theme_minimal() + -->
<!--   labs(x = "Time (s)", y = "") -->
<!-- ``` -->



<!-- ### Home keygram sections -->

<!-- ```{r} -->
<!-- Home_keygram2 |> -->
<!--   compmus_match_pitch_template( -->
<!--     chord_templates,         # Change to chord_templates if descired -->
<!--     method = "euclidean",  # Try different distance metrics -->
<!--     norm = "manhattan"     # Try different norms -->
<!--   ) |> -->
<!--   ggplot( -->
<!--     aes(x = start + duration / 2, width = duration, y = name, fill = d) -->
<!--   ) + -->
<!--   geom_tile() + -->
<!--   scale_fill_viridis_c(guide = "none") + -->
<!--   theme_minimal() + -->
<!--   labs(x = "Time (s)", y = "") -->
<!-- ``` -->

<!-- New tab (2) -->
<!-- ===================================== -->


<!-- column_hist {data-width=750} -->
<!-- -------------------------------------------------- -->

<!-- ### histogram -->
<!-- ```{r} -->
<!-- ggplot(data = my_portfolio_key, aes(x = key)) + geom_histogram(binwidth = 1, fill = "skyblue", color = "black", alpha = 1, ) + scale_x_continuous(breaks = unique(my_portfolio_key$key), labels = unique(my_portfolio_key$key)) + labs(title = "Histogram Key", x = "Key", y = "Frequency") -->



<!-- ``` -->



<!-- column_hist2 {data-width=250} -->
<!-- -------------------------------------------------- -->


<!-- ### Text -->

<!-- On the left side you can see a histogram of the key of all my most listened songs of 2022 -->




<!-- New tab (3) -->
<!-- ===================================== -->


<!-- column_test {data-width=750} -->
<!-- -------------------------------------------------- -->

<!-- ### Nice Plot -->
<!-- ```{r} -->
<!-- # MeestBeluisterd2022 <- -->
<!-- #   get_playlist_audio_features( -->
<!-- #     "thesoundsofspotify", -->
<!-- #     "0nYReIA8L6ep3yZp00TLwf" -->
<!-- #   ) |> -->
<!-- #   slice(1:30) |> -->
<!-- #   add_audio_analysis() -->
<!-- # -->
<!-- # Home <- -->
<!-- #   get_playlist_audio_features( -->
<!-- #     "thesoundsofspotify", -->
<!-- #     "2OKDjAF0WFX2OntThq1N6f" -->
<!-- #   ) |> -->
<!-- #   slice(1:30) |> -->
<!-- #   add_audio_analysis() -->
<!-- # -->
<!-- # Home_new <- Home |> mutate(genre = "Home") -->
<!-- # -->
<!-- # MeestBeluisterd2022 <- MeestBeluisterd2022 |> mutate(genre = "MeestBeluisterd2022") -->
<!-- # -->
<!-- # NummerEnBeluisterd <- -->
<!-- #   MeestBeluisterd2022 |> -->
<!-- #   mutate(genre = "MeestBeluisterd2022") |> -->
<!-- #   bind_rows(Home |> mutate(genre = "Home")) -->
<!-- # -->
<!-- # q <- MeestBeluisterd2022 |> -->
<!-- #   mutate( -->
<!-- #     timbre = -->
<!-- #       map( -->
<!-- #         segments, -->
<!-- #         compmus_summarise, -->
<!-- #         timbre, -->
<!-- #         method = "mean" -->
<!-- #       ) -->
<!-- #   ) |> -->
<!-- #   select(genre, timbre) |> -->
<!-- #   compmus_gather_timbre() |> -->
<!-- #   filter(basis %in% c("c01", "c02", "c03", "c04", "c05", "c06")) |> -->
<!-- #   ggplot(aes(x = basis, y = value, fill = "peachpuff")) + -->
<!-- #   geom_violin(fill = "peachpuff") + -->
<!-- #   scale_fill_viridis_d() + -->
<!-- #   labs(x = "Spotify Timbre Coefficients", y = "") -->
<!-- # -->
<!-- # -->
<!-- # # Add the Home data as points on top of the violins -->
<!-- # dingetjedong <- q + geom_point(data = Home_new %>% -->
<!-- #                  mutate( -->
<!-- #                    timbre = map( -->
<!-- #                      segments, -->
<!-- #                      compmus_summarise, -->
<!-- #                      timbre, -->
<!-- #                      method = "mean" -->
<!-- #                    ) -->
<!-- #                  ) %>% -->
<!-- #                  select(genre, timbre) %>% -->
<!-- #                  compmus_gather_timbre() %>% -->
<!-- #                  filter(basis %in% c("c01", "c02", "c03", "c04", "c05", "c06")), -->
<!-- #                aes(x = basis, y = value, fill = "peachpuff4"), -->
<!-- #                size = 2) -->
<!-- # -->
<!-- # dingetjedongetje <- dingetjedong + -->
<!-- #   guides(fill = FALSE) + theme_bw() -->
<!-- # -->
<!-- # ggplotly(dingetjedongetje, tooltip = c("value")) -->

<!-- ``` -->



<!-- column_test2 {data-width=250} -->
<!-- -------------------------------------------------- -->


<!-- ### Tekst -->
<!-- On the left, you can see violin plots of the timbre of all the songs in my most listened playlist of 2022, compared to my own song. They are shown for the first 6 layers of the timbre computing functions of Spotify. -->





<!-- Old tabs {.storyboard} -->
<!-- ===================================== -->

<!-- ### Chromatogram -->

<!-- #### The Chromagram and cepstogram of my piece: -->

<!-- ```{r} -->
<!-- Home_tidy <- -->
<!--   get_tidy_audio_analysis("2YonDH4haZzgbo3NjpIalE") |> -->
<!--   select(segments) |> -->
<!--   unnest(segments) |> -->
<!--   select(start, duration, pitches) -->

<!-- Home_tidy |> -->
<!--   mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |> -->
<!--   compmus_gather_chroma() |> -->
<!--   ggplot( -->
<!--     aes( -->
<!--       x = start + duration / 2, -->
<!--       width = duration, -->
<!--       y = pitch_class, -->
<!--       fill = value -->
<!--     ) -->
<!--   ) + -->
<!--   geom_tile() + -->
<!--   labs(x = "Time (s)", y = NULL, fill = "Magnitude") + -->
<!--   theme_minimal() + -->
<!--   scale_fill_viridis_c() -->

<!-- Home_Cepstogram <- -->
<!--   get_tidy_audio_analysis("2YonDH4haZzgbo3NjpIalE") |> # Change URI. -->
<!--   compmus_align(bars, segments) |>                     # Change `bars` -->
<!--   select(bars) |>                                      #   in all three -->
<!--   unnest(bars) |>                                      #   of these lines. -->
<!--   mutate( -->
<!--     pitches = -->
<!--       map(segments, -->
<!--         compmus_summarise, pitches, -->
<!--         method = "rms", norm = "euclidean"              # Change summary & norm. -->
<!--       ) -->
<!--   ) |> -->
<!--   mutate( -->
<!--     timbre = -->
<!--       map(segments, -->
<!--         compmus_summarise, timbre, -->
<!--         method = "rms", norm = "euclidean"              # Change summary & norm. -->
<!--       ) -->
<!--   ) -->

<!-- Home_Cepstogram |> -->
<!--   compmus_gather_timbre() |> -->
<!--   ggplot( -->
<!--     aes( -->
<!--       x = start + duration / 2, -->
<!--       width = duration, -->
<!--       y = basis, -->
<!--       fill = value -->
<!--     ) -->
<!--   ) + -->
<!--   geom_tile() + -->
<!--   labs(x = "Time (s)", y = NULL, fill = "Magnitude") + -->
<!--   scale_fill_viridis_c() + -->
<!--   theme_classic() -->


<!-- ``` -->

<!-- ------------------------------------------------------------------------ -->

<!-- On the **left side** you can see the chromagram of my song "Home". Overall it looks pretty clear to distinguish the different pitch classes that are present in the piece. Although I would say the piece is not in one particular key, the verses are in Cminor. The first chorus starts around 50 seconds into the song, and there you can see the C is no longer prevalent in the song. In the outro of the song, starting around 120 seconds in, the song rests on the Cminor chord for a long time and that can be clearly extracted from this chromatogram. -->

<!-- Then on the **right side** a cepstogram of my song can be seen. I found that by using a "euclidean" normalisation  and the "root mean square" as a summary statistic, I got the clearest representation. There are three things that to me are interesting to see in this cepstogram. -->

<!-- - In the first layer c01, which is basically the loudness of the piece at a given time, you can see that on the far left- and right side, the song is very quiet. You can recognise this by the darkness of the blocks. -->
<!-- - In the second layer c02, which shows the presence of lower frequencies (bass), the sections between 0 and 25 seconds and between 60 and 80 seconds look brighter. I can't really explain it that well, other than that it could be assigned to the fact that the loudness of the bass is there the highest, relative to the rest of the piece. -->
<!-- - In the third layer c03, which expresses the presence of the mids, you can see that between 25 and 50 seconds in this layer gets brighter. I think this is because that's where the vocals come in. -->


<!-- ### Self-similarity matrice (New tab) -->

<!-- #### The Self-similarity matrice of my song: -->

<!-- ```{r} -->
<!-- Home_self_sim <- -->
<!--   get_tidy_audio_analysis("2YonDH4haZzgbo3NjpIalE") |> # Change URI. -->
<!--   compmus_align(bars, segments) |>                     # Change `bars` -->
<!--   select(bars) |>                                      #   in all three -->
<!--   unnest(bars) |>                                      #   of these lines. -->
<!--   mutate( -->
<!--     pitches = -->
<!--       map(segments, -->
<!--         compmus_summarise, pitches, -->
<!--         method = "rms", norm = "euclidean"              # Change summary & norm. -->
<!--       ) -->
<!--   ) |> -->
<!--   mutate( -->
<!--     timbre = -->
<!--       map(segments, -->
<!--         compmus_summarise, timbre, -->
<!--         method = "rms", norm = "euclidean"              # Change summary & norm. -->
<!--       ) -->
<!--   ) -->


<!-- bind_rows( -->
<!--   Home_self_sim |> -->
<!--     compmus_self_similarity(pitches, "euclidean") |> -->
<!--     mutate(d = d / max(d), type = "Chroma"), -->
<!--   Home_self_sim |> -->
<!--     compmus_self_similarity(timbre, "euclidean") |> -->
<!--     mutate(d = d / max(d), type = "Timbre") -->
<!-- ) |> -->
<!--   mutate() |> -->
<!--   ggplot( -->
<!--     aes( -->
<!--       x = xstart + xduration / 2, -->
<!--       width = xduration, -->
<!--       y = ystart + yduration / 2, -->
<!--       height = yduration, -->
<!--       fill = d -->
<!--     ) -->
<!--   ) + -->
<!--   geom_tile() + -->
<!--   coord_fixed() + -->
<!--   facet_wrap(~type) + -->
<!--   scale_fill_viridis_c(guide = "none") + -->
<!--   theme_classic() + -->
<!--   labs(x = "", y = "") -->

<!-- ``` -->

<!-- *** -->
<!-- On this tab, you can see both the chroma-based and timbre-based self-similarity matrices of my song. Again, for both I liked using the "euclidean" and the "root mean square" the most. Now I will describe what I see in both matrices. -->

<!-- ##### **Chroma-based** -->
<!-- So based on the looking at the left matrix, you can generally distinguish three different kind of sections. -->
<!-- - Verses: Between both 0 and 45 seconds and 65 and 110 seconds you can se repeating checkerboard like patterns. These are the verses. They make use of the same chord progressions. -->
<!-- - Choruses: Between 45 and 65 and between 110 and 130 you can see the choruses. Although they don't really show diagonals clearly, the same pattern can be seen in both sections. -->
<!-- - Outro: From around 130 seconds in, the outro starts. This is represented in the top right of the matrix and a clear black checkerboard box can be seen. -->




<!-- ### Introduction -->

<!-- (I just changed my topic 2 days ago, so let me know what you think.) -->

<!-- Just three days ago, the debut single, "Home", of my band was released on Spotify, marking a significant moment in my journey as a songwriter. It's the first time that the public can openly listen to my music, and I'm feeling a mix of excitement and nerves as I await the response from listeners. -->

<!-- But the release of my song on Spotify also offers new possibilities beyond just reaching a wider audience. With the help of the Spotify API, I can analyze my song and gain insights into its musical characteristics. -->

<!-- As a songwriter, I'm always fascinated by the creative process and how other music shapes the songs we write. For this project, I'm curious to explore the ways in which different artists and genres have influenced my own music. -->

<!-- To do this, I plan to compare my debut song to my top songs of 2022 and identify any correlations or similarities. Which artists or songs have had the greatest impact on my writing, and have I unintentionally incorporated elements of other music into my work. In other words: have I maybe accidentally ripped someone off? -->

<!-- As a songwriter, I believe that understanding the creative influences behind our music can enhance our artistry and lead to new discoveries. This project offers a unique opportunity to delve deeper into the musical elements that shape my writing and explore the ways in which other artists and genres have impacted my work. I'm excited to embark on this journey of self-discovery and to share my findings with fellow music enthusiasts. -->

<!-- If you stumble upon this dashboard and you would like to see what the song, "Home", sounds like, here is the Spotify link: <https://open.spotify.com/track/2YonDH4haZzgbo3NjpIalE?si=324e6a2d4e424358> -->

<!-- ### Comparing numerical variables -->

<!-- ```{r} -->
<!-- ding <- ggplot(jahe.long,aes(x=variables,value,fill=variable))+ -->
<!--      geom_bar(stat="identity",position="dodge") + theme(axis.text.x = element_text(angle = 90)) -->

<!-- ggplotly(ding) -->
<!-- ``` -->

<!-- ------------------------------------------------------------------------ -->

<!-- In the bar graph in red you can see the mean for all the variables of my 101 top songs of 2022. These variables are the ones that are ranked between 0 and 1 by Spotify. In blue you see these numbers for my song. They differ the most for accounstichness. -->



